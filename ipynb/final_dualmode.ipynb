{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08a11649-75f3-4e38-b76f-38b379d69bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter notebook helper: import another .ipynb as module\n",
    "import sys, os, pathlib, types\n",
    "\n",
    "def import_ipynb(ipynb_path: str):\n",
    "    \"\"\"\n",
    "    다른 .ipynb 파일을 모듈처럼 불러오기 위한 헬퍼.\n",
    "    예: utils = import_ipynb(\"RF_LGBM.ipynb\")\n",
    "    \"\"\"\n",
    "    import nbformat\n",
    "    from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "    ipynb_path = os.path.abspath(ipynb_path)\n",
    "    if not os.path.exists(ipynb_path):\n",
    "        raise FileNotFoundError(f\"ipynb module not found: {ipynb_path}\")\n",
    "\n",
    "    nb = nbformat.read(ipynb_path, as_version=4)\n",
    "    shell = InteractiveShell.instance()\n",
    "    module_name = pathlib.Path(ipynb_path).stem\n",
    "    mod = types.ModuleType(module_name)\n",
    "    mod.__file__ = ipynb_path\n",
    "    sys.modules[module_name] = mod\n",
    "\n",
    "    for cell in nb.cells:\n",
    "        if cell.cell_type == \"code\":\n",
    "            code = shell.input_transformer_manager.transform_cell(cell.source)\n",
    "            exec(code, mod.__dict__)\n",
    "\n",
    "    return mod\n",
    "\n",
    "# 워킹 디렉토리를 이 노트북 위치로 맞춰두는 게 안전함\n",
    "os.chdir(os.path.dirname(os.path.abspath(\"__file__\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1eebeb9-e653-48eb-a668-ca64a61d6594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, warnings\n",
    "from typing import Tuple\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# plotting\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score, precision_recall_curve, auc,\n",
    "    roc_curve, roc_auc_score,\n",
    "    precision_score, recall_score, f1_score\n",
    ")\n",
    "\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb53ed69-3dd0-4e8d-958b-86de498b7e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "# Default Config\n",
    "# ------------------------\n",
    "CFG = dict(\n",
    "    data_path=\"dataset/train_features_60m.csv\",\n",
    "    probs_path_candidates=[\"dataset/holdout_probs.csv\", \"holdout_probs.csv\"],\n",
    "    ts_col=\"datetime\",\n",
    "    label_col=\"passorfail\",\n",
    "    id_cols=[\"_grp\"],\n",
    "    # window/label settings (mirror the user's run)\n",
    "    seq_len=300, stride=1,\n",
    "    label_mode=\"pre_event\",\n",
    "    label_delta_minutes=180,\n",
    "    pos_dilate_back_steps=6,\n",
    "    resample=\"\",\n",
    "    # split\n",
    "    val_holdout_days=1.5,\n",
    "    embargo_minutes=30,\n",
    "    min_pos_train=10,\n",
    "    min_pos_holdout=300,\n",
    "    target_train_frac=0.7,\n",
    "    train_frac_weight=10.0,\n",
    "    # policy / thresholds\n",
    "    post_k=4,\n",
    "    open_th=0.57,\n",
    "    close_th=0.33,\n",
    "    cooldown_min=120,\n",
    "    # soft-vote weights\n",
    "    w_tab=0.6,\n",
    "    w_dl=0.4,\n",
    "    # cascade gate\n",
    "    gate_th=0.5,\n",
    "    # selection constraint\n",
    "    precision_floor=0.90,\n",
    "    # output\n",
    "    out_root=\"evals_ensemble\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5f80650-b8ab-40ce-94ed-792ca082775e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] imports loaded.\n",
      "[OK] labeling & auto-feature funcs ready.\n",
      "[OK] sequence builder ready.\n",
      "[OK] split funcs ready.\n",
      "[OK] event-based split ready.\n",
      "[OK] dataset & sampler ready.\n",
      "[OK] model & loss defined.\n",
      "[OK] postprocess & eval helpers ready.\n",
      "[OK] train/eval functions ready.\n",
      "[DEBUG] total samples=17,265 | positives=2,286\n",
      "[SPLIT] cutoff=2020-10-30 13:08:44 | embargo=30min | train_win=8806 (pos=1599) | holdout_win=7502 (pos=328) | OK (nudged)\n",
      "[PREP] drop zero-variance cols on train: 18 dropped\n",
      "[SPLIT-FINAL] train windows=8,806 (pos=1,599) | holdout windows=7,502 (pos=328)\n",
      "[INFO] Focal params: gamma=2.0, alpha_pos=0.8184, pos_weight=4.507\n",
      "Epoch 001 | train_loss=0.022143 | AP=0.955787\n",
      "Epoch 002 | train_loss=0.001164 | AP=0.934540\n",
      "Epoch 003 | train_loss=0.000512 | AP=0.881917\n",
      "[EARLY-STOP] patience=2, best_AP=0.955787\n",
      "[SAVE] holdout probabilities → dataset/holdout_probs.csv\n",
      "[EVALS] Saved: evals/*.png, metrics.txt\n",
      "[SAVE] model artifacts (single file) → models/model_artifacts.pkl\n",
      "[DONE] training pipeline completed.\n"
     ]
    }
   ],
   "source": [
    "# ------------------------\n",
    "# User notebooks import (TCN, RF/LGBM)\n",
    "# ------------------------\n",
    "\n",
    "# 네가 말한 이름 그대로 씀\n",
    "TCN_NOTEBOOK = \"TCN.ipynb\"\n",
    "TAB_NOTEBOOK = \"RF_LGBM.ipynb\"\n",
    "\n",
    "# TCN 유틸 가져오기\n",
    "tcn = import_ipynb(TCN_NOTEBOOK)\n",
    "\n",
    "# 필요하면 탭모델 관련 유틸도 불러둘 수 있음\n",
    "# tab_mod = import_ipynb(TAB_NOTEBOOK)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc17a7e1-8dc8-4543-946a-701bbeb3f45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_base_dataframe(path: str, ts_col: str, label_col: str, id_cols: list) -> Tuple[pd.DataFrame, list]:\n",
    "    df = pd.read_csv(path)\n",
    "    if ts_col not in df.columns:\n",
    "        raise ValueError(f\"ts_col '{ts_col}' not found in {path}\")\n",
    "    if label_col not in df.columns:\n",
    "        raise ValueError(f\"label_col '{label_col}' not found in {path}\")\n",
    "    df[ts_col] = pd.to_datetime(df[ts_col])\n",
    "    df = df.sort_values(ts_col).reset_index(drop=True)\n",
    "    excl = set([ts_col, label_col] + (id_cols or []))\n",
    "    feat_cols = [c for c in df.columns if c not in excl and pd.api.types.is_numeric_dtype(df[c])]\n",
    "    if not feat_cols:\n",
    "        raise ValueError(\"No numeric feature columns found for tabular models.\")\n",
    "    return df, feat_cols\n",
    "\n",
    "\n",
    "def make_labels_and_windows(tcn, df: pd.DataFrame, feat_cols: list, cfg: dict):\n",
    "    if cfg[\"resample\"]:\n",
    "        tmp = df.set_index(cfg[\"ts_col\"])\n",
    "        agg = {c: \"mean\" for c in feat_cols}; agg[cfg[\"label_col\"]] = \"max\"\n",
    "        df = tmp.resample(cfg[\"resample\"]).agg(agg).reset_index()\n",
    "        excl = set([cfg[\"ts_col\"], cfg[\"label_col\"]] + (cfg[\"id_cols\"] or []))\n",
    "        feat_cols = [c for c in df.columns if c not in excl and pd.api.types.is_numeric_dtype(df[c])]\n",
    "\n",
    "    if cfg[\"label_mode\"] != \"pre_event\":\n",
    "        raise NotImplementedError(\"only pre_event supported\")\n",
    "    y = tcn.make_pre_event_labels(\n",
    "        df, cfg[\"ts_col\"], cfg[\"label_col\"],\n",
    "        delta_min=cfg[\"label_delta_minutes\"],\n",
    "        pos_dilate_back_steps=cfg[\"pos_dilate_back_steps\"],\n",
    "        resample_minutes=1\n",
    "    )\n",
    "\n",
    "    seqs_all, X_dummy, ts_min = tcn.build_windows(\n",
    "        df, cfg[\"ts_col\"], feat_cols, y,\n",
    "        seq_len=cfg[\"seq_len\"], stride=cfg[\"stride\"],\n",
    "        pre_event_minutes=cfg[\"label_delta_minutes\"]\n",
    "    )\n",
    "\n",
    "    tr_mask_pts, ho_mask_pts = tcn.embargo_posaware_split(\n",
    "        df, cfg[\"ts_col\"], y,\n",
    "        holdout_days=cfg[\"val_holdout_days\"], embargo_minutes=cfg[\"embargo_minutes\"],\n",
    "        min_pos_train=cfg[\"min_pos_train\"], min_pos_holdout=cfg[\"min_pos_holdout\"],\n",
    "        seqs_for_split=seqs_all, min_train_windows=32, min_holdout_windows=32,\n",
    "        target_train_frac=cfg[\"target_train_frac\"], train_frac_weight=cfg[\"train_frac_weight\"]\n",
    "    )\n",
    "    tr_idx = [i for i, s in enumerate(seqs_all) if tr_mask_pts[s.start] and tr_mask_pts[s.end - 1]]\n",
    "    ho_idx = [i for i, s in enumerate(seqs_all) if ho_mask_pts[s.start] and ho_mask_pts[s.end - 1]]\n",
    "    win_end_row = np.array([s.end - 1 for s in seqs_all], dtype=int)\n",
    "    ho_end_rows = win_end_row[ho_idx]\n",
    "\n",
    "    return df, feat_cols, y, seqs_all, tr_idx, ho_idx, ho_end_rows\n",
    "\n",
    "\n",
    "def _select_feature_columns(df: pd.DataFrame, model, cfg):\n",
    "    if hasattr(model, \"feature_names_in_\"):\n",
    "        cols = [c for c in model.feature_names_in_ if c in df.columns]\n",
    "        if cols: return cols\n",
    "    if hasattr(model, \"booster_\") and hasattr(model.booster_, \"feature_name\"):\n",
    "        try:\n",
    "            names = list(model.booster_.feature_name())\n",
    "            cols = [c for c in names if c in df.columns]\n",
    "            if cols: return cols\n",
    "        except Exception:\n",
    "            pass\n",
    "    excl = set([cfg[\"ts_col\"], cfg[\"label_col\"]] + (cfg[\"id_cols\"] or []))\n",
    "    return [c for c in df.columns if c not in excl and pd.api.types.is_numeric_dtype(df[c])]\n",
    "\n",
    "\n",
    "def predict_proba_tabular(df_hold: pd.DataFrame, cfg: dict):\n",
    "    rf_path  = \"models/model_RF_calibrated.pkl\" if Path(\"models/model_RF_calibrated.pkl\").exists() else \"model_RF_calibrated.pkl\"\n",
    "    lgb_path = \"models/model_LightGBM.pkl\"      if Path(\"models/model_LightGBM.pkl\").exists()      else \"model_LightGBM.pkl\"\n",
    "    models = []\n",
    "    if Path(rf_path).exists():\n",
    "        try: models.append(joblib.load(rf_path))\n",
    "        except Exception as e: warnings.warn(f\"RF load failed: {e}\")\n",
    "    if Path(lgb_path).exists():\n",
    "        try: models.append(joblib.load(lgb_path))\n",
    "        except Exception as e: warnings.warn(f\"LGBM load failed: {e}\")\n",
    "    if not models:\n",
    "        raise FileNotFoundError(\"No tabular models could be loaded.\")\n",
    "\n",
    "    feat_sets = [set(_select_feature_columns(df_hold, m, cfg)) for m in models]\n",
    "    common_feats = set.intersection(*feat_sets) if feat_sets else set()\n",
    "    if not common_feats: common_feats = feat_sets[0]\n",
    "    cols = sorted(list(common_feats))\n",
    "\n",
    "    X = df_hold[cols].copy()\n",
    "    imp = SimpleImputer(strategy=\"median\")\n",
    "    X_imp_arr = imp.fit_transform(X)\n",
    "    X_imp = pd.DataFrame(X_imp_arr, columns=X.columns, index=X.index)\n",
    "\n",
    "    probs = []\n",
    "    for m in models:\n",
    "        p = m.predict_proba(X_imp)\n",
    "        if p.ndim == 2 and p.shape[1] == 2:\n",
    "            p = p[:, 1]\n",
    "        probs.append(p.astype(float))\n",
    "    P_tab = np.mean(np.vstack(probs), axis=0) if len(probs) > 1 else probs[0]\n",
    "    return P_tab, cols\n",
    "\n",
    "\n",
    "def hysteresis_cooldown(prob: np.ndarray, win_ends_ts: np.ndarray,\n",
    "                        open_th: float, close_th: float, cooldown_min: int) -> np.ndarray:\n",
    "    assert prob.shape[0] == len(win_ends_ts)\n",
    "    y = np.zeros(len(prob), dtype=int)\n",
    "    state = 0; last_alarm_end_ts = None\n",
    "    for i, p in enumerate(prob):\n",
    "        t = win_ends_ts[i]\n",
    "        if state == 0:\n",
    "            ok_cool = True\n",
    "            if last_alarm_end_ts is not None:\n",
    "                ok_cool = (t >= last_alarm_end_ts + np.timedelta64(int(cooldown_min), 'm'))\n",
    "            if ok_cool and (p >= open_th):\n",
    "                state = 1; y[i] = 1\n",
    "        else:\n",
    "            y[i] = 1\n",
    "            if p <= close_th:\n",
    "                state = 0\n",
    "                last_alarm_end_ts = t\n",
    "    return y\n",
    "\n",
    "\n",
    "def k_consecutive_filter(yhat_bin: np.ndarray, k: int) -> np.ndarray:\n",
    "    if k <= 1: return yhat_bin\n",
    "    y = yhat_bin.copy(); n = len(y); i = 0\n",
    "    while i < n:\n",
    "        if y[i] == 1:\n",
    "            j = i\n",
    "            while j < n and y[j] == 1: j += 1\n",
    "            if (j - i) < k: y[i:j] = 0\n",
    "            i = j\n",
    "        else: i += 1\n",
    "    return y\n",
    "\n",
    "\n",
    "def save_eval_plots(y_true, p_tab, p_dl, p_ens, outdir: Path, tag: str):\n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "    # PR\n",
    "    for name, p in [(\"tab\", p_tab), (\"dl\", p_dl), (tag, p_ens)]:\n",
    "        prec, rec, _ = precision_recall_curve(y_true, p)\n",
    "        au = auc(rec, prec) if len(rec) > 1 else float(\"nan\")\n",
    "        plt.figure()\n",
    "        plt.step(rec, prec, where=\"post\")\n",
    "        base = (np.asarray(y_true)==1).mean()\n",
    "        plt.hlines(base, 0, 1, linestyles=\"dashed\")\n",
    "        plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\")\n",
    "        plt.title(f\"PR Curve [{name}] AUC={au:.4f} (baseline={base:.3f})\")\n",
    "        plt.grid(True, alpha=0.3); plt.tight_layout()\n",
    "        plt.savefig(outdir / f\"pr_{name}.png\", dpi=150); plt.close()\n",
    "    # ROC\n",
    "    for name, p in [(\"tab\", p_tab), (\"dl\", p_dl), (tag, p_ens)]:\n",
    "        if (np.max(y_true) != np.min(y_true)):\n",
    "            fpr, tpr, _ = roc_curve(y_true, p)\n",
    "            aucv = roc_auc_score(y_true, p)\n",
    "            plt.figure()\n",
    "            plt.plot(fpr, tpr, label=f\"AUC={aucv:.4f}\")\n",
    "            plt.plot([0,1],[0,1],\"--\")\n",
    "            plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\")\n",
    "            plt.title(f\"ROC Curve [{name}]\")\n",
    "            plt.legend(); plt.grid(True, alpha=0.3); plt.tight_layout()\n",
    "            plt.savefig(outdir / f\"roc_{name}.png\", dpi=150); plt.close()\n",
    "    # Hist\n",
    "    for name, p in [(\"tab\", p_tab), (\"dl\", p_dl), (tag, p_ens)]:\n",
    "        plt.figure()\n",
    "        y = np.asarray(y_true).astype(int)\n",
    "        if (y==1).any(): plt.hist(p[y==1], bins=30, alpha=0.6, label=\"pos\")\n",
    "        if (y==0).any(): plt.hist(p[y==0], bins=30, alpha=0.6, label=\"neg\")\n",
    "        plt.xlabel(\"Predicted probability\"); plt.ylabel(\"Count\")\n",
    "        plt.title(f\"Probability Histogram [{name}]\"); plt.legend(); plt.tight_layout()\n",
    "        plt.savefig(outdir / f\"hist_{name}.png\", dpi=150); plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fdd7e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SPLIT] cutoff=2020-10-30 13:08:44 | embargo=30min | train_win=8806 (pos=1599) | holdout_win=7502 (pos=328) | OK (nudged)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.0\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[CASCADE] gate_th=0.550 | suspicious=287/7502\n",
      "[CASCADE] Saved plots & metrics to evals_ensemble/cascade\n",
      "[SPLIT] cutoff=2020-10-30 13:08:44 | embargo=30min | train_win=8806 (pos=1599) | holdout_win=7502 (pos=328) | OK (nudged)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] lambda_l2 is set=2.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=2.0\n",
      "[LightGBM] [Warning] lambda_l1 is set=1.0, reg_alpha=0.0 will be ignored. Current value: lambda_l1=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[SOFT] Saved plots & metrics to evals_ensemble/soft\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "def run_pipeline(mode: str,\n",
    "                 w_tab: float = None,\n",
    "                 w_dl: float = None,\n",
    "                 gate_th: float = None,\n",
    "                 precision_floor: float = 0.90,\n",
    "                 out_root: str = \"evals_ensemble\"):\n",
    "    cfg = CFG.copy()\n",
    "    cfg[\"precision_floor\"] = precision_floor\n",
    "    cfg[\"out_root\"] = out_root\n",
    "\n",
    "    if mode == \"soft\":\n",
    "        assert w_tab is not None and w_dl is not None\n",
    "        cfg[\"w_tab\"] = w_tab\n",
    "        cfg[\"w_dl\"] = w_dl\n",
    "    elif mode == \"cascade\":\n",
    "        assert gate_th is not None\n",
    "        cfg[\"gate_th\"] = gate_th\n",
    "    else:\n",
    "        raise ValueError(f\"unknown mode: {mode}\")\n",
    "\n",
    "    outdir = Path(out_root) / mode\n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    df, feat_cols = load_base_dataframe(\n",
    "        cfg[\"data_path\"], cfg[\"ts_col\"], cfg[\"label_col\"], cfg[\"id_cols\"]\n",
    "    )\n",
    "\n",
    "    df2, feat_cols2, y, seqs_all, tr_idx, ho_idx, ho_end_rows = make_labels_and_windows(\n",
    "        tcn, df.copy(), feat_cols, dict(\n",
    "            ts_col=cfg[\"ts_col\"], label_col=cfg[\"label_col\"], id_cols=cfg[\"id_cols\"],\n",
    "            resample=cfg[\"resample\"],\n",
    "            label_mode=cfg[\"label_mode\"],\n",
    "            label_delta_minutes=cfg[\"label_delta_minutes\"],\n",
    "            pos_dilate_back_steps=cfg[\"pos_dilate_back_steps\"],\n",
    "            seq_len=cfg[\"seq_len\"], stride=cfg[\"stride\"],\n",
    "            val_holdout_days=cfg[\"val_holdout_days\"],\n",
    "            embargo_minutes=cfg[\"embargo_minutes\"],\n",
    "            min_pos_train=cfg[\"min_pos_train\"], min_pos_holdout=cfg[\"min_pos_holdout\"],\n",
    "            target_train_frac=cfg[\"target_train_frac\"], train_frac_weight=cfg[\"train_frac_weight\"],\n",
    "        )\n",
    "    )\n",
    "\n",
    "    ts_vals = df2[cfg[\"ts_col\"]].values.astype(\"datetime64[ns]\")\n",
    "    ho_end_ts = ts_vals[ho_end_rows]\n",
    "\n",
    "    probs_path = None\n",
    "    for cand in cfg[\"probs_path_candidates\"]:\n",
    "        if Path(cand).exists():\n",
    "            probs_path = cand\n",
    "            break\n",
    "    if probs_path is None:\n",
    "        raise FileNotFoundError(\"holdout_probs.csv not found in candidates: \" + \", \".join(cfg[\"probs_path_candidates\"]))\n",
    "    tcn_df = pd.read_csv(probs_path)\n",
    "    p_dl = tcn_df[\"p_hat\"].to_numpy() if \"p_hat\" in tcn_df.columns else tcn_df.iloc[:, -1].to_numpy()\n",
    "\n",
    "    y_hold = np.array([seqs_all[i].label for i in ho_idx], dtype=int)\n",
    "\n",
    "    n = min(len(p_dl), len(y_hold))\n",
    "    if len(p_dl) != len(y_hold):\n",
    "        warnings.warn(f\"[{mode}] Length mismatch: {len(p_dl)} vs {len(y_hold)} → trunc {n}\")\n",
    "    p_dl = p_dl[:n]\n",
    "    y_hold = y_hold[:n]\n",
    "    ho_end_ts = ho_end_ts[:n]\n",
    "    hold_rows = ho_end_rows[:n]\n",
    "    df_hold = df2.iloc[hold_rows].copy()\n",
    "\n",
    "    p_tab, used_cols = predict_proba_tabular(df_hold, cfg)\n",
    "    p_tab = p_tab[:n]\n",
    "\n",
    "    if mode == \"cascade\":\n",
    "        mask = (p_dl >= cfg[\"gate_th\"]).astype(float)\n",
    "        p_ens = mask * p_tab\n",
    "        tag = \"cascade\"\n",
    "        print(f\"[{tag.upper()}] gate_th={cfg['gate_th']:.3f} | suspicious={int(mask.sum())}/{len(mask)}\")\n",
    "    else:\n",
    "        p_ens = (cfg[\"w_tab\"] * p_tab + cfg[\"w_dl\"] * p_dl).astype(float)\n",
    "        tag = \"soft\"\n",
    "\n",
    "    yb_hys = hysteresis_cooldown(\n",
    "        p_ens, ho_end_ts,\n",
    "        cfg[\"open_th\"], cfg[\"close_th\"], cfg[\"cooldown_min\"]\n",
    "    )\n",
    "    yb = k_consecutive_filter(yb_hys, cfg[\"post_k\"])\n",
    "\n",
    "    from sklearn.metrics import average_precision_score, precision_recall_curve, precision_score, recall_score, f1_score\n",
    "\n",
    "    ap_tab = average_precision_score(y_hold, p_tab)\n",
    "    ap_dl  = average_precision_score(y_hold, p_dl)\n",
    "    ap_ens = average_precision_score(y_hold, p_ens)\n",
    "\n",
    "    prec, rec, thr = precision_recall_curve(y_hold, p_ens)\n",
    "    thr = np.append(thr, 1.0)\n",
    "    best_f1, best_t = -1.0, None\n",
    "    for t in thr:\n",
    "        yy = (p_ens >= t).astype(int)\n",
    "        yy = k_consecutive_filter(yy, cfg[\"post_k\"])\n",
    "        P = precision_score(y_hold, yy, zero_division=0)\n",
    "        R = recall_score(y_hold, yy, zero_division=0)\n",
    "        if cfg[\"precision_floor\"] is not None and P < cfg[\"precision_floor\"]:\n",
    "            continue\n",
    "        f1v = (2 * P * R / (P + R)) if (P + R) > 0 else 0.0\n",
    "        if f1v > best_f1:\n",
    "            best_f1, best_t = f1v, t\n",
    "\n",
    "    save_eval_plots(y_hold, p_tab, p_dl, p_ens, outdir, tag)\n",
    "\n",
    "    lines = []\n",
    "    lines.append(f\"Mode: {mode}\")\n",
    "    lines.append(f\"AP_tab: {ap_tab:.6f}\")\n",
    "    lines.append(f\"AP_dl: {ap_dl:.6f}\")\n",
    "    lines.append(f\"AP_{tag}: {ap_ens:.6f}\")\n",
    "    if best_t is not None:\n",
    "        yy = (p_ens >= best_t).astype(int)\n",
    "        yy = k_consecutive_filter(yy, cfg[\"post_k\"])\n",
    "        P = precision_score(y_hold, yy, zero_division=0)\n",
    "        R = recall_score(y_hold, yy, zero_division=0)\n",
    "        F = f1_score(y_hold, yy, zero_division=0)\n",
    "        lines.append(f\"Best_threshold_under_precision_floor({cfg['precision_floor']}): {best_t:.6f}\")\n",
    "        lines.append(f\"Precision@best: {P:.6f}\")\n",
    "        lines.append(f\"Recall@best: {R:.6f}\")\n",
    "        lines.append(f\"F1@best: {F:.6f}\")\n",
    "    else:\n",
    "        lines.append(\"No threshold met precision_floor\")\n",
    "\n",
    "    with open(outdir / f\"metrics_{tag}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(lines))\n",
    "\n",
    "    dbg = pd.DataFrame({\n",
    "        \"y_true\": y_hold,\n",
    "        \"p_tab\": p_tab,\n",
    "        \"p_dl\": p_dl,\n",
    "        \"p_ens\": p_ens,\n",
    "        cfg[\"ts_col\"]: ho_end_ts.astype(\"datetime64[ns]\")\n",
    "    })\n",
    "    dbg.to_csv(outdir / f\"holdout_probs_{tag}.csv\", index=False)\n",
    "\n",
    "    print(f\"[{tag.upper()}] Saved plots & metrics to {outdir}\")\n",
    "\n",
    "\n",
    "# 실제 실행: 원래 CLI에서 두 번 때리던 거 그대로 노트북에서 한 번에\n",
    "run_pipeline(\n",
    "    mode=\"cascade\",\n",
    "    gate_th=0.55,\n",
    "    precision_floor=0.90,\n",
    "    out_root=\"evals_ensemble\",\n",
    ")\n",
    "\n",
    "run_pipeline(\n",
    "    mode=\"soft\",\n",
    "    w_tab=0.7,\n",
    "    w_dl=0.3,\n",
    "    precision_floor=0.90,\n",
    "    out_root=\"evals_ensemble\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc7db37-d67a-4eb0-a96b-77d43c335685",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d804b62a-93e5-49e7-85cd-869c8054eb2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch 2.4 (GPU)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
