{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6caaea59",
   "metadata": {},
   "source": [
    "# Deep TCN Training Notebook (from `train_deep_tcn_clean_v2_evals.py`)\n",
    "\n",
    "- 원본: `train_deep_tcn_clean_v2_evals.py`\n",
    "- 목적: CLI 스크립트를 주피터 노트북 셀 구조로 변환해 과제로 제출 가능하게 함.\n",
    "- 주의: **모델/로직/하이퍼파라미터 정의는 원본과 동일한 순서로 유지**했고, 실행은 마지막 셀에서 `sys.argv` 수동 세팅 후 `main()` 호출로 대체.\n",
    "- 경로(`dataset/...`, `models/...`, `evals/...`)는 원본과 동일하므로, 필요하면 폴더만 먼저 만들어 둘 것."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fd542f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] imports loaded.\n"
     ]
    }
   ],
   "source": [
    "import os, argparse, math, random\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, f1_score, precision_score, recall_score\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")  # notebook에서도 파일로 저장되게\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# global flip helper\n",
    "FLIP_LOGITS = 1  # default\n",
    "\n",
    "def maybe_flip_logits(logits, args=None):\n",
    "    try:\n",
    "        flip = int(getattr(args, \"flip_logits\", FLIP_LOGITS))\n",
    "    except Exception:\n",
    "        flip = FLIP_LOGITS\n",
    "    return -logits if flip == 1 else logits\n",
    "\n",
    "print(\"[OK] imports loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4d3aa83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Utils\n",
    "# ----------------------------\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def ts_to_minutes(ts: pd.Series) -> np.ndarray:\n",
    "    ns = ts.to_numpy(dtype=\"datetime64[ns]\").astype(\"int64\")\n",
    "    return (ns // 10**9) // 60\n",
    "\n",
    "def human_count(n):\n",
    "    return f\"{n:,}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1116555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] labeling & auto-feature funcs ready.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Labeling\n",
    "# ----------------------------\n",
    "def make_pre_event_labels(df: pd.DataFrame, ts_col: str, label_col: str,\n",
    "                          delta_min: int, pos_dilate_back_steps: int,\n",
    "                          resample_minutes: int) -> np.ndarray:\n",
    "    y = np.zeros(len(df), dtype=np.int8)\n",
    "    ts_min = ts_to_minutes(df[ts_col])\n",
    "    pos_idx = np.where(df[label_col].values.astype(int) == 1)[0]\n",
    "    if len(pos_idx) == 0:\n",
    "        return y\n",
    "    pos_ts = ts_min[pos_idx]\n",
    "    delta = int(delta_min)\n",
    "    for pt in pos_ts:\n",
    "        start_min = pt - delta\n",
    "        mask = (ts_min >= start_min) & (ts_min <= pt)\n",
    "        y[mask] = 1\n",
    "    if pos_dilate_back_steps > 0:\n",
    "        pos_where = np.where(y == 1)[0]\n",
    "        for k in range(1, pos_dilate_back_steps + 1):\n",
    "            prev_idx = pos_where - k\n",
    "            y[prev_idx[prev_idx >= 0]] = 1\n",
    "    return y\n",
    "\n",
    "# ----------------------------\n",
    "# Auto Features (past-only)\n",
    "# ----------------------------\n",
    "def build_auto_features(df: pd.DataFrame, feature_cols: list, ts_col: str, enable: bool):\n",
    "    if not enable:\n",
    "        return df, feature_cols\n",
    "\n",
    "    base = df[feature_cols].copy()\n",
    "    feats = [base]\n",
    "\n",
    "    for k in (1, 5, 15):\n",
    "        d = base.diff(k); d.columns = [f\"{c}_d{k}\" for c in d.columns]; feats.append(d)\n",
    "    for k in (1, 5):\n",
    "        p = base.pct_change(k); p.columns = [f\"{c}_pct{k}\" for c in p.columns]; feats.append(p)\n",
    "    for w in (10, 30, 60):\n",
    "        rstd = base.rolling(w, min_periods=2).std(); rstd.columns = [f\"{c}_rstd{w}\" for c in rstd.columns]; feats.append(rstd)\n",
    "        rmax = base.rolling(w, min_periods=2).max(); rmin = base.rolling(w, min_periods=2).min()\n",
    "        rrange = rmax - rmin; rrange.columns = [f\"{c}_rrange{w}\" for c in rrange.columns]; feats.append(rrange)\n",
    "    diff1 = base.diff(1).abs(); ema_span = 10\n",
    "    ema = diff1.ewm(span=ema_span, adjust=False).mean(); ema.columns = [f\"{c}_ema_absdiff1_{ema_span}\" for c in ema.columns]\n",
    "    feats.append(ema)\n",
    "\n",
    "    g = pd.concat(feats, axis=1); g.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df_aug = pd.concat([df.drop(columns=feature_cols), g], axis=1)\n",
    "    new_cols = list(g.columns)\n",
    "    return df_aug, new_cols\n",
    "\n",
    "print(\"[OK] labeling & auto-feature funcs ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f7d77aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] sequence builder ready.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Sequence Builder\n",
    "# ----------------------------\n",
    "@dataclass\n",
    "class SeqIndex:\n",
    "    start: int; end: int; label: int; nearest_event_dist_min: int\n",
    "\n",
    "def build_windows(df: pd.DataFrame, ts_col: str, feature_cols: list,\n",
    "                  labels: np.ndarray, seq_len: int, stride: int,\n",
    "                  pre_event_minutes: int):\n",
    "    X = df[feature_cols].values.astype(np.float32)\n",
    "    ts_min = ts_to_minutes(df[ts_col])\n",
    "    pos_idx = np.where(labels == 1)[0]\n",
    "    event_ts = ts_min[pos_idx] if len(pos_idx) else np.array([], dtype=np.int64)\n",
    "\n",
    "    seqs = []; N = len(df); i = 0\n",
    "    while i + seq_len <= N:\n",
    "        s, e = i, i + seq_len\n",
    "        y = 1 if labels[s:e].max() == 1 else 0\n",
    "        if len(event_ts):\n",
    "            end_min = ts_min[e - 1]\n",
    "            future_diffs = event_ts - end_min\n",
    "            future_diffs = future_diffs[future_diffs >= 0]\n",
    "            nearest = int(future_diffs.min()) if len(future_diffs) else 10**9\n",
    "        else:\n",
    "            nearest = 10**9\n",
    "        seqs.append(SeqIndex(s, e, int(y), nearest))\n",
    "        i += stride\n",
    "    return seqs, X, ts_min\n",
    "\n",
    "print(\"[OK] sequence builder ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b56a8c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] split funcs ready.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Time / Event split (pos-aware)\n",
    "# ----------------------------\n",
    "def embargo_posaware_split(df: pd.DataFrame, ts_col: str, labels: np.ndarray,\n",
    "                           holdout_days: float, embargo_minutes: int,\n",
    "                           min_pos_train: int, min_pos_holdout: int, *,\n",
    "                           seqs_for_split, min_train_windows: int = 32,\n",
    "                           min_holdout_windows: int = 32,\n",
    "                           target_train_frac: float = 0.7,\n",
    "                           train_frac_weight: float = 10.0):\n",
    "    ts = df[ts_col]\n",
    "    cutoff_base = ts.max() - pd.Timedelta(days=float(holdout_days))\n",
    "\n",
    "    idx_to_ts = ts.values\n",
    "    win_starts = np.array([idx_to_ts[s.start] for s in seqs_for_split], dtype=\"datetime64[ns]\")\n",
    "    win_ends   = np.array([idx_to_ts[s.end - 1] for s in seqs_for_split], dtype=\"datetime64[ns]\")\n",
    "    win_labels = np.array([s.label for s in seqs_for_split], dtype=int)\n",
    "\n",
    "    candidates = []\n",
    "    lo = ts.min() + pd.Timedelta(hours=2)\n",
    "    hi = ts.max() - pd.Timedelta(hours=2)\n",
    "    for shift_min in range(-72*60, 72*60 + 1, 15):\n",
    "        c = cutoff_base + pd.Timedelta(minutes=shift_min)\n",
    "        if c < lo or c > hi: continue\n",
    "        candidates.append(c)\n",
    "\n",
    "    def counts_at_cut(cut):\n",
    "        tr_mask_w = (win_ends <= (cut.to_datetime64() - np.timedelta64(embargo_minutes, 'm')))\n",
    "        ho_mask_w = (win_starts >= cut.to_datetime64())\n",
    "        tr_n = int(tr_mask_w.sum()); ho_n = int(ho_mask_w.sum())\n",
    "        tr_pos = int(win_labels[tr_mask_w].sum()); ho_pos = int(win_labels[ho_mask_w].sum())\n",
    "        return tr_mask_w, ho_mask_w, tr_n, ho_n, tr_pos, ho_pos\n",
    "\n",
    "    def eval_cut(cut):\n",
    "        tr_m, ho_m, tr_n, ho_n, tr_pos, ho_pos = counts_at_cut(cut)\n",
    "        feasible = (tr_n >= min_train_windows) and (ho_n >= min_holdout_windows) and \\\n",
    "                   (tr_pos >= min_pos_train) and (ho_pos >= min_pos_holdout)\n",
    "        train_frac = tr_n / max(1, (tr_n + ho_n))\n",
    "        score = (10.0 * ho_pos + 5.0 * tr_pos\n",
    "                 - train_frac_weight * abs(train_frac - target_train_frac) * (tr_n + ho_n)\n",
    "                 - (abs(tr_n - min_train_windows) + abs(ho_n - min_holdout_windows)))\n",
    "        return feasible, score, (cut, tr_m, ho_m, tr_n, ho_n, tr_pos, ho_pos, train_frac)\n",
    "\n",
    "    best_pack, best_score, best_feasible = None, -1e18, False\n",
    "    for c in candidates:\n",
    "        feasible, score, pack = eval_cut(c)\n",
    "        if feasible and (score > best_score):\n",
    "            best_score, best_pack, best_feasible = score, pack, True\n",
    "        elif (not best_feasible) and (score > best_score):\n",
    "            best_score, best_pack, best_feasible = score, pack, False\n",
    "\n",
    "    if best_pack is None:\n",
    "        raise RuntimeError(\"No candidate cut evaluated.\")\n",
    "\n",
    "    cut, tr_m, ho_m, tr_n, ho_n, tr_pos, ho_pos, train_frac = best_pack\n",
    "    moved = False; step = pd.Timedelta(minutes=5); max_steps = 200\n",
    "    if train_frac < target_train_frac:\n",
    "        for _ in range(max_steps):\n",
    "            c2 = cut + step\n",
    "            tr_m2, ho_m2, tr_n2, ho_n2, tr_pos2, ho_pos2 = counts_at_cut(c2)\n",
    "            feasible2 = (tr_n2 >= min_train_windows) and (ho_n2 >= min_holdout_windows) and \\\n",
    "                        (tr_pos2 >= min_pos_train) and (ho_pos2 >= min_pos_holdout)\n",
    "            if not feasible2: break\n",
    "            train_frac2 = tr_n2 / max(1, (tr_n2 + ho_n2))\n",
    "            if train_frac2 <= train_frac: break\n",
    "            cut, tr_m, ho_m, tr_n, ho_n, tr_pos, ho_pos, train_frac = c2, tr_m2, ho_m2, tr_n2, ho_n2, tr_pos2, ho_pos2, train_frac2\n",
    "            moved = True\n",
    "            if train_frac >= target_train_frac: break\n",
    "    elif train_frac > (target_train_frac + 0.02):\n",
    "        for _ in range(max_steps):\n",
    "            c2 = cut - step\n",
    "            tr_m2, ho_m2, tr_n2, ho_n2, tr_pos2, ho_pos2 = counts_at_cut(c2)\n",
    "            feasible2 = (tr_n2 >= min_train_windows) and (ho_n2 >= min_holdout_windows) and \\\n",
    "                        (tr_pos2 >= min_pos_train) and (ho_pos2 >= min_pos_holdout)\n",
    "            if not feasible2: break\n",
    "            train_frac2 = tr_n2 / max(1, (tr_n2 + ho_n2))\n",
    "            if train_frac2 >= train_frac: break\n",
    "            cut, tr_m, ho_m, tr_n, ho_n, tr_pos, ho_pos, train_frac = c2, tr_m2, ho_m2, tr_n2, ho_n2, tr_pos2, ho_pos2, train_frac2\n",
    "            moved = True\n",
    "            if train_frac <= target_train_frac: break\n",
    "\n",
    "    status = \"OK\" if best_feasible else \"WARN\"\n",
    "    note = \" (nudged)\" if moved else \"\"\n",
    "    print(f\"[SPLIT] cutoff={cut} | embargo={embargo_minutes}min | train_win={tr_n} (pos={tr_pos}) | holdout_win={ho_n} (pos={ho_pos}) | {status}{note}\")\n",
    "\n",
    "    train_mask_pts = df[ts_col].values <= (cut.to_datetime64() - np.timedelta64(embargo_minutes, 'm'))\n",
    "    hold_mask_pts  = df[ts_col].values >=  cut.to_datetime64()\n",
    "\n",
    "    if tr_pos == 0: print(\"[WARN] Train has 0 positives after correction.\")\n",
    "    if ho_pos == 0: print(\"[WARN] Holdout has 0 positives after correction.\")\n",
    "    return train_mask_pts, hold_mask_pts\n",
    "\n",
    "print(\"[OK] split funcs ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4221f5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] event-based split ready.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Event-based split\n",
    "# ----------------------------\n",
    "def event_based_split(df: pd.DataFrame, ts_col: str, labels: np.ndarray, seqs_for_split: list,\n",
    "                      pre_event_minutes: int, holdout_frac: float = 0.3, holdout_min_events: int = 4,\n",
    "                      seed: int = 42, extra_min: int = 0, target_holdout_frac: float = 0.0,\n",
    "                      target_holdout_max: float = 0.5, pos_dilate_back_steps: int = 0, tail_min: int = 30):\n",
    "    y = labels.astype(int)\n",
    "    nxt = np.concatenate((y[1:], [0]))\n",
    "    ends_idx = np.where((y == 1) & (nxt == 0))[0]\n",
    "    if len(ends_idx) == 0:\n",
    "        print(\"[WARN] No events found for event-based split. Fallback to time split.\")\n",
    "        return None, None\n",
    "\n",
    "    m = len(ends_idx)\n",
    "    k = max(holdout_min_events, int(round(m * holdout_frac)))\n",
    "    k = min(k, m)\n",
    "    rng = np.random.default_rng(seed)\n",
    "    ho_events = rng.choice(ends_idx, size=k, replace=False)\n",
    "\n",
    "    ts = df[ts_col].values\n",
    "    ts_min = (ts.astype(\"datetime64[s]\").astype(\"int64\")) // 60\n",
    "\n",
    "    dilate_extra = max(0, int(pos_dilate_back_steps))\n",
    "    delta = int(pre_event_minutes) + dilate_extra\n",
    "    extra = max(0, int(extra_min))\n",
    "    tail = max(1, int(tail_min))\n",
    "    tail = min(tail, delta + extra)\n",
    "\n",
    "    ho_point_mask = np.zeros(len(df), dtype=bool)\n",
    "    for eidx in ho_events:\n",
    "        t_e = ts_min[eidx]\n",
    "        mask = (ts_min >= (t_e - tail - extra)) & (ts_min <= (t_e + extra))\n",
    "        ho_point_mask |= mask\n",
    "\n",
    "    tr_idx, ho_idx = [], []\n",
    "    for i, s in enumerate(seqs_for_split):\n",
    "        rows = np.arange(s.start, s.end)\n",
    "        if ho_point_mask[rows].any():\n",
    "            ho_idx.append(i)\n",
    "        else:\n",
    "            tr_idx.append(i)\n",
    "\n",
    "    total_win = len(seqs_for_split)\n",
    "    if target_holdout_frac and total_win > 0:\n",
    "        cap = min(max(0.0, target_holdout_max), 0.95)\n",
    "        tgt = min(cap, max(0.0, float(target_holdout_frac)))\n",
    "        need = int(round(total_win * tgt)) - len(ho_idx)\n",
    "        if need > 0:\n",
    "            neg_train_idx = [i for i in tr_idx if seqs_for_split[i].label == 0]\n",
    "            if len(neg_train_idx) > 0:\n",
    "                add = rng.choice(neg_train_idx, size=min(need, len(neg_train_idx)), replace=False).tolist()\n",
    "                for a in add:\n",
    "                    tr_idx.remove(a)\n",
    "                ho_idx.extend(add)\n",
    "\n",
    "    y_tr = np.array([seqs_for_split[i].label for i in tr_idx], dtype=int)\n",
    "    y_ho = np.array([seqs_for_split[i].label for i in ho_idx], dtype=int)\n",
    "    print(f\"[SPLIT-EVENT] events_total={m} | events_holdout={k} | train_win={len(tr_idx)} (pos={int(y_tr.sum())}) | holdout_win={len(ho_idx)} (pos={int(y_ho.sum())})\")\n",
    "    return np.array(tr_idx, dtype=int), np.array(ho_idx, dtype=int)\n",
    "\n",
    "print(\"[OK] event-based split ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ad50053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] dataset & sampler ready.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Sampler / Dataset\n",
    "# ----------------------------\n",
    "class HardNegPosSampler(Sampler):\n",
    "    def __init__(self, seqs, batch_size=128, pos_oversample=8, hard_neg_frac=0.3, neg_frac=0.3, shuffle=True):\n",
    "        self.batch_size = batch_size; self.shuffle = shuffle\n",
    "        self.pos_idx = [i for i, s in enumerate(seqs) if s.label == 1]\n",
    "        self.neg_idx = [i for i, s in enumerate(seqs) if s.label == 0]\n",
    "        hard_thresh = 90\n",
    "        self.hard_neg_idx = [i for i, s in enumerate(seqs) if (s.label == 0 and s.nearest_event_dist_min <= hard_thresh)]\n",
    "        self.other_neg_idx = list(set(self.neg_idx) - set(self.hard_neg_idx))\n",
    "\n",
    "        self.hard_neg_quota = int(round(self.batch_size * hard_neg_frac))\n",
    "        self.neg_quota = int(round(self.batch_size * neg_frac))\n",
    "        if self.hard_neg_quota > self.neg_quota: self.hard_neg_quota = self.neg_quota\n",
    "        self.other_neg_quota = max(0, self.neg_quota - self.hard_neg_quota)\n",
    "        self.pos_quota = max(1, self.batch_size - (self.hard_neg_quota + self.other_neg_quota))\n",
    "        self.pos_oversample = int(max(1, pos_oversample))\n",
    "\n",
    "        self.hard_pool = self.hard_neg_idx.copy(); self.other_pool = self.other_neg_idx.copy()\n",
    "        if self.shuffle:\n",
    "            random.shuffle(self.hard_pool); random.shuffle(self.other_pool)\n",
    "\n",
    "        denom = max(1, (self.hard_neg_quota + self.other_neg_quota))\n",
    "        self.num_batches = max(1, math.ceil(len(self.neg_idx) / denom))\n",
    "\n",
    "    def __len__(self): return self.num_batches\n",
    "\n",
    "    def __iter__(self):\n",
    "        for _ in range(self.num_batches):\n",
    "            batch = []\n",
    "            if len(self.hard_pool) < self.hard_neg_quota:\n",
    "                self.hard_pool = self.hard_neg_idx.copy()\n",
    "                if self.shuffle: random.shuffle(self.hard_pool)\n",
    "            batch += self.hard_pool[:self.hard_neg_quota]\n",
    "            self.hard_pool = self.hard_pool[self.hard_neg_quota:]\n",
    "\n",
    "            if len(self.other_pool) < self.other_neg_quota:\n",
    "                self.other_pool = self.other_neg_idx.copy()\n",
    "                if self.shuffle: random.shuffle(self.other_pool)\n",
    "            batch += self.other_pool[:self.other_neg_quota]\n",
    "            self.other_pool = self.other_pool[self.other_neg_quota:]\n",
    "\n",
    "            if len(self.pos_idx) == 0:\n",
    "                neg_pool = self.hard_neg_idx + self.other_neg_idx\n",
    "                pos_take = [] if len(neg_pool) == 0 else np.random.choice(neg_pool, size=self.pos_quota, replace=True).tolist()\n",
    "            else:\n",
    "                pos_take = np.random.choice(self.pos_idx, size=self.pos_quota, replace=True).tolist()\n",
    "            batch += pos_take\n",
    "\n",
    "            if self.shuffle: random.shuffle(batch)\n",
    "            yield batch\n",
    "\n",
    "class WindowDataset(Dataset):\n",
    "    def __init__(self, X: np.ndarray, seqs: list):\n",
    "        self.X = X; self.seqs = seqs\n",
    "    def __len__(self): return len(self.seqs)\n",
    "    def __getitem__(self, idx):\n",
    "        s = self.seqs[idx]\n",
    "        x = self.X[s.start:s.end]; y = s.label\n",
    "        return torch.from_numpy(x), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "print(\"[OK] dataset & sampler ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3cd754c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] model & loss defined.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Models (GRU / TCN) + Losses\n",
    "# ----------------------------\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, in_dim, hidden=128, layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_size=in_dim, hidden_size=hidden, num_layers=layers,\n",
    "                          batch_first=True, bidirectional=False)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, hidden // 2), nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden // 2, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        out, _ = self.gru(x)\n",
    "        h = out[:, -1, :]\n",
    "        return self.head(h).squeeze(1)\n",
    "\n",
    "class TCNBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k=3, d=1, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.k, self.d = k, d\n",
    "        self.conv1 = nn.Conv1d(in_ch, out_ch, kernel_size=k, dilation=d, padding=0, bias=True)\n",
    "        self.conv2 = nn.Conv1d(out_ch, out_ch, kernel_size=k, dilation=d, padding=0, bias=True)\n",
    "        self.relu = nn.ReLU(); self.drop = nn.Dropout(p=dropout)\n",
    "        self.down = nn.Conv1d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
    "    def _causal_pad(self, x):\n",
    "        pad = (self.k - 1) * self.d\n",
    "        return F.pad(x, (pad, 0))\n",
    "    def forward(self, x):\n",
    "        y = self.conv1(self._causal_pad(x)); y = self.relu(y); y = self.drop(y)\n",
    "        y = self.conv2(self._causal_pad(y)); y = self.relu(y); y = self.drop(y)\n",
    "        return y + self.down(x)\n",
    "\n",
    "class TCNModel(nn.Module):\n",
    "    def __init__(self, in_dim, channels=(64, 64, 64), dropout=0.2):\n",
    "        super().__init__()\n",
    "        layers = []; c_in = in_dim; d = 1\n",
    "        for c in channels:\n",
    "            layers.append(TCNBlock(c_in, c, k=3, d=d, dropout=dropout))\n",
    "            c_in = c; d *= 2\n",
    "        self.tcn = nn.Sequential(*layers)\n",
    "        self.head = nn.Linear(c_in, 1)\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "        y = self.tcn(x)\n",
    "        last = y[:, :, -1]\n",
    "        return self.head(last).squeeze(1)\n",
    "\n",
    "class FocalLossWithLogits(nn.Module):\n",
    "    def __init__(self, gamma=2.0, alpha_pos=0.5, label_smoothing=0.0, pos_weight=None):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma; self.alpha_pos = alpha_pos; self.label_smoothing = label_smoothing\n",
    "        self.register_buffer(\"pos_weight\", None if pos_weight is None else torch.tensor(pos_weight, dtype=torch.float32))\n",
    "    def forward(self, logits, targets):\n",
    "        if self.label_smoothing > 0:\n",
    "            eps = self.label_smoothing\n",
    "            targets = targets * (1 - eps) + 0.5 * eps\n",
    "        bce = nn.functional.binary_cross_entropy_with_logits(logits, targets, reduction='none', pos_weight=self.pos_weight)\n",
    "        p = torch.sigmoid(logits); y = targets\n",
    "        alpha_t = self.alpha_pos * y + (1 - self.alpha_pos) * (1 - y)\n",
    "        p_t = p * y + (1 - p) * (1 - y)\n",
    "        focal_weight = alpha_t * torch.pow((1 - p_t).clamp(1e-6, 1.0), self.gamma)\n",
    "        return (focal_weight * bce).mean()\n",
    "\n",
    "def batch_pairwise_rank_loss(logits, targets, margin=0.2):\n",
    "    with torch.no_grad():\n",
    "        pos_mask = targets > 0.5\n",
    "        neg_mask = targets < 0.5\n",
    "    if (pos_mask.sum() == 0) or (neg_mask.sum() == 0):\n",
    "        return logits.new_zeros(())\n",
    "    pos_scores = logits[pos_mask]\n",
    "    neg_scores = logits[neg_mask]\n",
    "    max_pairs = 4096\n",
    "    P = pos_scores.numel(); N = neg_scores.numel()\n",
    "    if P * N == 0: return logits.new_zeros(())\n",
    "    k_neg = min(128, N)\n",
    "    neg_topk, _ = torch.topk(neg_scores, k_neg)\n",
    "    k_pos = min(256, P)\n",
    "    pos_sel = pos_scores if P <= k_pos else torch.topk(pos_scores, k_pos).values\n",
    "    diff = pos_sel.view(-1,1) - neg_topk.view(1,-1)\n",
    "    loss = torch.relu(margin - diff)\n",
    "    if loss.numel() > max_pairs:\n",
    "        idx = torch.randperm(loss.numel(), device=loss.device)[:max_pairs]\n",
    "        loss = loss.view(-1)[idx]\n",
    "    return loss.mean()\n",
    "\n",
    "def aug_mixup(x, y, alpha=0.0):\n",
    "    if alpha <= 0 or len(x) < 2: return x, y\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    perm = torch.randperm(x.size(0), device=x.device)\n",
    "    x2 = x[perm]; y2 = y[perm]\n",
    "    return lam * x + (1 - lam) * x2, lam * y + (1 - lam) * y2\n",
    "\n",
    "def aug_jitter(x, std=0.0):\n",
    "    if std <= 0: return x\n",
    "    return x + torch.randn_like(x) * std\n",
    "\n",
    "def aug_time_cutout(x, prob=0.0):\n",
    "    if prob <= 0: return x\n",
    "    B, T, F_ = x.shape\n",
    "    for b in range(B):\n",
    "        if random.random() < prob:\n",
    "            k = max(1, int(T * random.uniform(0.05, 0.1)))\n",
    "            start = random.randint(0, T - k)\n",
    "            x[b, start:start + k, :] = 0.0\n",
    "    return x\n",
    "\n",
    "print(\"[OK] model & loss defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2051eb85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] postprocess & eval helpers ready.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Postprocess & calibration\n",
    "# ----------------------------\n",
    "def k_consecutive_filter(yhat_bin: np.ndarray, k: int) -> np.ndarray:\n",
    "    if k <= 1: return yhat_bin\n",
    "    y = yhat_bin.copy(); n = len(y); i = 0\n",
    "    while i < n:\n",
    "        if y[i] == 1:\n",
    "            j = i\n",
    "            while j < n and y[j] == 1: j += 1\n",
    "            if (j - i) < k: y[i:j] = 0\n",
    "            i = j\n",
    "        else: i += 1\n",
    "    return y\n",
    "\n",
    "def ema_series(x: np.ndarray, span: int) -> np.ndarray:\n",
    "    if span <= 1: return x.copy()\n",
    "    alpha = 2.0 / (span + 1.0)\n",
    "    y = np.zeros_like(x, dtype=float); y[0] = x[0]\n",
    "    for i in range(1, len(x)):\n",
    "        y[i] = alpha * x[i] + (1 - alpha) * y[i - 1]\n",
    "    return y\n",
    "\n",
    "def hysteresis_cooldown(prob: np.ndarray, win_ends_ts: np.ndarray,\n",
    "                        open_th: float, close_th: float, cooldown_min: int) -> np.ndarray:\n",
    "    assert prob.shape[0] == len(win_ends_ts)\n",
    "    y = np.zeros(len(prob), dtype=int)\n",
    "    state = 0; last_alarm_end_ts = None\n",
    "    for i, p in enumerate(prob):\n",
    "        t = win_ends_ts[i]\n",
    "        if state == 0:\n",
    "            ok_cool = True\n",
    "            if last_alarm_end_ts is not None:\n",
    "                ok_cool = (t >= last_alarm_end_ts + np.timedelta64(int(cooldown_min), 'm'))\n",
    "            if ok_cool and (p >= open_th):\n",
    "                state = 1; y[i] = 1\n",
    "        else:\n",
    "            y[i] = 1\n",
    "            if p <= close_th:\n",
    "                state = 0\n",
    "                last_alarm_end_ts = t\n",
    "    return y\n",
    "\n",
    "def _safe_logit(p):\n",
    "    eps = 1e-6\n",
    "    p = np.clip(p, eps, 1 - eps)\n",
    "    return np.log(p / (1 - p))\n",
    "\n",
    "def calibrate_probs(y_prob, y_true, method=\"none\"):\n",
    "    if method == \"none\":\n",
    "        return y_prob\n",
    "    y_true = y_true.astype(int)\n",
    "    if method == \"platt\":\n",
    "        X = _safe_logit(y_prob).reshape(-1, 1)\n",
    "        lr = LogisticRegression(max_iter=1000)\n",
    "        lr.fit(X, y_true)\n",
    "        return lr.predict_proba(X)[:, 1]\n",
    "    if method == \"isotonic\":\n",
    "        iso = IsotonicRegression(out_of_bounds=\"clip\")\n",
    "        iso.fit(y_prob, y_true)\n",
    "        return iso.transform(y_prob)\n",
    "    return y_prob\n",
    "\n",
    "def choose_threshold_with_k(y_true, y_prob, k=1, recall_floor=None, precision_floor=None, fp_budget=None):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_prob)\n",
    "    thresholds = np.append(thresholds, 1.0)\n",
    "    best_f1, chosen = -1.0, None\n",
    "    fp_allowed = fp_budget if fp_budget is not None else 10**9\n",
    "    for t in thresholds:\n",
    "        yhat = (y_prob >= t).astype(int)\n",
    "        yhat = k_consecutive_filter(yhat, k)\n",
    "        tp = int(((yhat == 1) & (y_true == 1)).sum())\n",
    "        fp = int(((yhat == 1) & (y_true == 0)).sum())\n",
    "        fn = int(((yhat == 0) & (y_true == 1)).sum())\n",
    "        p = tp / max(1, (tp + fp)); r = tp / max(1, (tp + fn))\n",
    "        if (recall_floor is not None and r < recall_floor) or \\\n",
    "           (precision_floor is not None and p < precision_floor) or \\\n",
    "           (fp > fp_allowed): continue\n",
    "        f1 = (2 * p * r / (p + r)) if (p + r) > 0 else 0.0\n",
    "        if f1 > best_f1: best_f1, chosen = f1, t\n",
    "    if chosen is None:\n",
    "        for t in thresholds:\n",
    "            yhat = (y_prob >= t).astype(int)\n",
    "            yhat = k_consecutive_filter(yhat, k)\n",
    "            tp = int(((yhat == 1) & (y_true == 1)).sum())\n",
    "            fp = int(((yhat == 1) & (y_true == 0)).sum())\n",
    "            fn = int(((yhat == 0) & (y_true == 1)).sum())\n",
    "            p = tp / max(1, (tp + fp)); r = tp / max(1, (tp + fn))\n",
    "            f1 = (2 * p * r / (p + r)) if (p + r) > 0 else 0.0\n",
    "            if f1 > best_f1: best_f1, chosen = f1, t\n",
    "    return chosen\n",
    "\n",
    "def _save_eval_plots_and_report(y_true, y_prob, args):\n",
    "    os.makedirs(\"evals\", exist_ok=True)\n",
    "    y = np.asarray(y_true).reshape(-1).astype(int)\n",
    "    p = np.asarray(y_prob).reshape(-1).astype(float)\n",
    "    m = np.isfinite(y) & np.isfinite(p)\n",
    "    y = y[m]; p = p[m]\n",
    "    report_lines = []\n",
    "\n",
    "    prec, rec, thr = precision_recall_curve(y, p)\n",
    "    from sklearn.metrics import auc\n",
    "    auprc_curve = auc(rec, prec) if (len(rec) > 1) else float(\"nan\")\n",
    "\n",
    "    plt.figure()\n",
    "    plt.step(rec, prec, where=\"post\")\n",
    "    base = (y == 1).mean() if len(y) else 0.0\n",
    "    plt.hlines(base, 0, 1, linestyles=\"dashed\")\n",
    "    plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\")\n",
    "    plt.title(f\"Precision-Recall Curve (baseline={base:.3f}, AUC={auprc_curve:.4f})\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout(); plt.savefig(\"evals/pr_curve.png\", dpi=150); plt.close()\n",
    "\n",
    "    roc_auc = float(\"nan\")\n",
    "    if (y.max() != y.min()):\n",
    "        fpr, tpr, _ = roc_curve(y, p)\n",
    "        roc_auc = roc_auc_score(y, p)\n",
    "        plt.figure(); plt.plot(fpr, tpr, label=f\"AUC={roc_auc:.4f}\")\n",
    "        plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "        plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"ROC Curve\")\n",
    "        plt.legend(); plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout(); plt.savefig(\"evals/roc_curve.png\", dpi=150); plt.close()\n",
    "\n",
    "    try:\n",
    "        frac_pos, mean_pred = calibration_curve(y, p, n_bins=10, strategy=\"uniform\")\n",
    "        plt.figure(); plt.plot(mean_pred, frac_pos, marker=\"o\")\n",
    "        plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "        plt.xlabel(\"Mean predicted value\"); plt.ylabel(\"Fraction of positives\")\n",
    "        plt.title(\"Calibration Curve (Reliability)\"); plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout(); plt.savefig(\"evals/calibration_curve.png\", dpi=150); plt.close()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    thr_sweep = np.unique(np.clip(np.concatenate([thr, [0.0, 1.0]]), 0, 1))\n",
    "    pr_list, re_list, f1_list = [], [], []\n",
    "    for t in thr_sweep:\n",
    "        yhat = (p >= t).astype(int)\n",
    "        if hasattr(args, \"post_k\") and int(args.post_k) > 1:\n",
    "            yhat = k_consecutive_filter(yhat, int(args.post_k))\n",
    "        tp = int(((yhat == 1) & (y == 1)).sum())\n",
    "        fp = int(((yhat == 1) & (y == 0)).sum())\n",
    "        fn = int(((yhat == 0) & (y == 1)).sum())\n",
    "        prec_t = tp / max(1, tp + fp); rec_t = tp / max(1, tp + fn)\n",
    "        f1_t = (2*prec_t*rec_t/max(1e-12, (prec_t + rec_t))) if (prec_t + rec_t) > 0 else 0.0\n",
    "        pr_list.append(prec_t); re_list.append(rec_t); f1_list.append(f1_t)\n",
    "\n",
    "    plt.figure(); plt.plot(thr_sweep, pr_list, label=\"Precision\")\n",
    "    plt.plot(thr_sweep, re_list, label=\"Recall\")\n",
    "    plt.plot(thr_sweep, f1_list, label=\"F1\")\n",
    "    plt.xlabel(\"Threshold\"); plt.ylabel(\"Score\")\n",
    "    plt.title(\"Threshold Sweep (Precision / Recall / F1)\")\n",
    "    plt.legend(); plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout(); plt.savefig(\"evals/threshold_sweep.png\", dpi=150); plt.close()\n",
    "\n",
    "    plt.figure()\n",
    "    if (y == 1).any(): plt.hist(p[y == 1], bins=30, alpha=0.6, label=\"pos\")\n",
    "    if (y == 0).any(): plt.hist(p[y == 0], bins=30, alpha=0.6, label=\"neg\")\n",
    "    plt.xlabel(\"Predicted probability\"); plt.ylabel(\"Count\")\n",
    "    plt.title(\"Probability Histogram by Class\"); plt.legend()\n",
    "    plt.tight_layout(); plt.savefig(\"evals/prob_hist.png\", dpi=150); plt.close()\n",
    "\n",
    "    try:\n",
    "        t_star = choose_threshold_with_k(\n",
    "            y_true=y, y_prob=p,\n",
    "            k=int(getattr(args, \"post_k\", 1)),\n",
    "            recall_floor=getattr(args, \"recall_floor\", None),\n",
    "            precision_floor=getattr(args, \"precision_floor\", None),\n",
    "            fp_budget=getattr(args, \"fp_budget\", None),\n",
    "        )\n",
    "    except Exception:\n",
    "        t_star = None\n",
    "    if (t_star is None) or (not np.isfinite(t_star)):\n",
    "        t_star = 0.5\n",
    "\n",
    "    yhat = (p >= t_star).astype(int)\n",
    "    if hasattr(args, \"post_k\") and int(args.post_k) > 1:\n",
    "        yhat = k_consecutive_filter(yhat, int(args.post_k))\n",
    "\n",
    "    prec_star = precision_score(y, yhat, zero_division=0)\n",
    "    rec_star  = recall_score(y, yhat, zero_division=0)\n",
    "    f1_star   = f1_score(y, yhat, zero_division=0)\n",
    "\n",
    "    report_lines.append(f\"AUPRC_curve: {auprc_curve:.6f}\")\n",
    "    report_lines.append(f\"ROC_AUC: {roc_auc:.6f}\" if (roc_auc == roc_auc) else \"ROC_AUC: nan\")\n",
    "    report_lines.append(f\"Chosen_threshold: {t_star:.6f}\")\n",
    "    report_lines.append(f\"Precision@th: {prec_star:.6f}\")\n",
    "    report_lines.append(f\"Recall@th: {rec_star:.6f}\")\n",
    "    report_lines.append(f\"F1@th: {f1_star:.6f}\")\n",
    "    report_lines.append(f\"post_k: {int(getattr(args, 'post_k', 1))}\")\n",
    "\n",
    "    with open(\"evals/metrics.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(report_lines))\n",
    "\n",
    "    print(\"[EVALS] Saved: evals/*.png, metrics.txt\")\n",
    "\n",
    "print(\"[OK] postprocess & eval helpers ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b5eea92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] train/eval functions ready.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Evaluate / Train loop\n",
    "# ----------------------------\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device, args=None):\n",
    "    model.eval()\n",
    "    ys, ps = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device); y = y.to(device)\n",
    "            logits = model(x)\n",
    "            logits = maybe_flip_logits(logits, args)\n",
    "            prob = torch.sigmoid(logits).squeeze(-1)\n",
    "            ys.append(y.detach().cpu().numpy())\n",
    "            ps.append(prob.detach().cpu().numpy())\n",
    "    if not ys:\n",
    "        return float('nan'), None, None\n",
    "    y = np.asarray(np.concatenate(ys)).reshape(-1).astype(int)\n",
    "    p = np.asarray(np.concatenate(ps)).reshape(-1).astype(float)\n",
    "    if np.nanmin(p) < 0 or np.nanmax(p) > 1:\n",
    "        p = 1.0 / (1.0 + np.exp(-p))\n",
    "    try:\n",
    "        mpos = float(np.nanmean(p[y==1])) if (y==1).any() else np.nan\n",
    "        mneg = float(np.nanmean(p[y==0])) if (y==0).any() else np.nan\n",
    "        if np.isfinite(mpos) and np.isfinite(mneg) and (mpos < mneg):\n",
    "            print(\"[WARN] mean(pos) < mean(neg) → flipping scores for evaluation\")\n",
    "            p = 1.0 - p\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        ap = average_precision_score(y, p)\n",
    "    except Exception:\n",
    "        ap = float('nan')\n",
    "    return ap, y, p\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, focal_loss_fn, device,\n",
    "                    mixup_alpha=0.0, jitter_std=0.0, time_cutout_prob=0.0,\n",
    "                    loss_mode=\"focal\", rank_weight=0.5, rank_margin=0.2, args=None):\n",
    "    model.train(); losses = []\n",
    "    for x, y in loader:\n",
    "        x = x.to(device); y = y.to(device)\n",
    "        x = aug_jitter(x, std=jitter_std)\n",
    "        x = aug_time_cutout(x, prob=time_cutout_prob)\n",
    "        x, y = aug_mixup(x, y, alpha=mixup_alpha)\n",
    "        logits = model(x)\n",
    "        logits = maybe_flip_logits(logits, args)\n",
    "        loss = focal_loss_fn(logits, y)\n",
    "        if \"rank\" in loss_mode:\n",
    "            rank_loss = batch_pairwise_rank_loss(logits.detach().clone(), y.detach().clone(), margin=rank_margin)\n",
    "            loss = (1 - rank_weight) * loss + rank_weight * rank_loss\n",
    "        optimizer.zero_grad(); loss.backward(); nn.utils.clip_grad_norm_(model.parameters(), 5.0); optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    return float(np.mean(losses)) if losses else 0.0\n",
    "\n",
    "print(\"[OK] train/eval functions ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad51554d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# MAIN (원본 구조 그대로, argparse만 notebook-friendly)\n",
    "# ----------------------------\n",
    "def parse_channels(ch_str: str):\n",
    "    try: return tuple(int(s.strip()) for s in ch_str.split(\",\") if s.strip())\n",
    "    except Exception: return (64, 64, 64)\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--input\", type=str, required=True)\n",
    "    parser.add_argument(\"--ts-col\", type=str, default=\"datetime\")\n",
    "    parser.add_argument(\"--id-cols\", nargs=\"*\", default=[\"_grp\"])\n",
    "    parser.add_argument(\"--label-col\", type=str, default=\"passorfail\")\n",
    "    parser.add_argument(\"--resample\", type=str, default=\"1min\")\n",
    "    parser.add_argument(\"--seq-len\", type=int, default=240)\n",
    "    parser.add_argument(\"--stride\", type=int, default=5)\n",
    "    parser.add_argument(\"--label-mode\", type=str, default=\"pre_event\", choices=[\"pre_event\"])\n",
    "    parser.add_argument(\"--label-delta-minutes\", type=int, default=60)\n",
    "    parser.add_argument(\"--pos-dilate-back-steps\", type=int, default=2)\n",
    "    parser.add_argument(\"--horizon-minutes\", type=int, default=15)\n",
    "    parser.add_argument(\"--val-holdout-days\", type=float, default=3.0)\n",
    "    parser.add_argument(\"--embargo-minutes\", type=int, default=30)\n",
    "    parser.add_argument(\"--min-pos-train\", type=int, default=10)\n",
    "    parser.add_argument(\"--min-pos-holdout\", type=int, default=2)\n",
    "    parser.add_argument(\"--target-train-frac\", type=float, default=0.7)\n",
    "    parser.add_argument(\"--train-frac-weight\", type=float, default=10.0)\n",
    "    parser.add_argument(\"--split-mode\", type=str, default=\"time\", choices=[\"time\", \"event\"])\n",
    "    parser.add_argument(\"--event-holdout-frac\", type=float, default=0.35)\n",
    "    parser.add_argument(\"--event-holdout-min\", type=int, default=4)\n",
    "    parser.add_argument(\"--event-holdout-extra-min\", type=int, default=0)\n",
    "    parser.add_argument(\"--target-holdout-frac\", type=float, default=0.0)\n",
    "    parser.add_argument(\"--target-holdout-max\", type=float, default=0.5)\n",
    "    parser.add_argument(\"--event-holdout-tail-min\", type=int, default=30)\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=128)\n",
    "    parser.add_argument(\"--epochs\", type=int, default=50)\n",
    "    parser.add_argument(\"--lr\", type=float, default=3e-4)\n",
    "    parser.add_argument(\"--weight-decay\", type=float, default=1e-4)\n",
    "    parser.add_argument(\"--model\", type=str, default=\"gru\", choices=[\"gru\", \"tcn\"])\n",
    "    parser.add_argument(\"--hidden\", type=int, default=128)\n",
    "    parser.add_argument(\"--layers\", type=int, default=2)\n",
    "    parser.add_argument(\"--tcn-channels\", type=str, default=\"64,64,64\")\n",
    "    parser.add_argument(\"--pos-oversample\", type=int, default=10)\n",
    "    parser.add_argument(\"--neg-frac\", type=float, default=0.3)\n",
    "    parser.add_argument(\"--hard-neg-frac\", type=float, default=0.3)\n",
    "    parser.add_argument(\"--gamma\", type=float, default=2.0)\n",
    "    parser.add_argument(\"--alpha\", type=float, default=-1.0)\n",
    "    parser.add_argument(\"--label-smoothing\", type=float, default=0.0)\n",
    "    parser.add_argument(\"--mixup-alpha\", type=float, default=0.0)\n",
    "    parser.add_argument(\"--jitter-std\", type=float, default=0.0)\n",
    "    parser.add_argument(\"--time-cutout-prob\", type=float, default=0.0)\n",
    "    parser.add_argument(\"--recall-floor\", type=float, default=0.95)\n",
    "    parser.add_argument(\"--precision-floor\", type=float, default=None)\n",
    "    parser.add_argument(\"--fp-budget\", type=int, default=10)\n",
    "    parser.add_argument(\"--use-best-ap\", action=\"store_true\", default=True)\n",
    "    parser.add_argument(\"--post-k\", type=int, default=1)\n",
    "    parser.add_argument(\"--event-eval\", action=\"store_true\", default=False)\n",
    "    parser.add_argument(\"--seed\", type=int, default=42)\n",
    "    parser.add_argument(\"--flip-logits\", type=int, default=1)\n",
    "    parser.add_argument(\"--use-raw-events\", action=\"store_true\", default=False)\n",
    "    parser.add_argument(\"--dropout\", type=float, default=0.2)\n",
    "    parser.add_argument(\"--early-stop-patience\", type=int, default=15)\n",
    "    parser.add_argument(\"--loss-mode\", type=str, default=\"focal\", choices=[\"focal\", \"focal+rank\"])\n",
    "    parser.add_argument(\"--rank-weight\", type=float, default=0.5)\n",
    "    parser.add_argument(\"--rank-margin\", type=float, default=0.2)\n",
    "    parser.add_argument(\"--ema-span\", type=int, default=0)\n",
    "    parser.add_argument(\"--open-th\", type=float, default=0.85)\n",
    "    parser.add_argument(\"--close-th\", type=float, default=0.65)\n",
    "    parser.add_argument(\"--cooldown-min\", type=int, default=0)\n",
    "    parser.add_argument(\"--auto-features\", type=int, default=0)\n",
    "    parser.add_argument(\"--save-probs\", type=int, default=1)\n",
    "    parser.add_argument(\"--probs-path\", type=str, default=\"dataset/holdout_probs.csv\")\n",
    "    parser.add_argument(\"--calibration\", type=str, default=\"none\", choices=[\"none\",\"platt\",\"isotonic\"])\n",
    "    parser.add_argument(\"--th-objective\", type=str, default=\"window_f1\",\n",
    "                        choices=[\"window_f1\",\"event_f1\",\"lead_p50\"])\n",
    "    parser.add_argument(\"--save-artifacts\", type=int, default=1)\n",
    "    parser.add_argument(\"--artifacts-path\", type=str, default=\"models/model_artifacts.pkl\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    set_seed(args.seed)\n",
    "\n",
    "    probs_dir = os.path.dirname(args.probs_path) or \".\"; os.makedirs(probs_dir, exist_ok=True)\n",
    "    art_dir = os.path.dirname(args.artifacts_path) or \".\"; os.makedirs(art_dir, exist_ok=True)\n",
    "\n",
    "    df = pd.read_csv(args.input)\n",
    "    if args.ts_col not in df.columns: raise ValueError(f\"ts_col {args.ts_col} not in data.\")\n",
    "    if args.label_col not in df.columns: raise ValueError(f\"label_col {args.label_col} not in data.\")\n",
    "    df[args.ts_col] = pd.to_datetime(df[args.ts_col])\n",
    "    df = df.sort_values(args.ts_col).reset_index(drop=True)\n",
    "\n",
    "    excl = set([args.ts_col, args.label_col] + (args.id_cols or []))\n",
    "    base_feature_cols = [c for c in df.columns if c not in excl and pd.api.types.is_numeric_dtype(df[c])]\n",
    "    if not base_feature_cols: raise ValueError(\"No numeric feature columns found.\")\n",
    "\n",
    "    if args.resample:\n",
    "        tmp = df.set_index(args.ts_col)\n",
    "        agg = {c: \"mean\" for c in base_feature_cols}; agg[args.label_col] = \"max\"\n",
    "        df = tmp.resample(args.resample).agg(agg).reset_index()\n",
    "\n",
    "    if args.label_mode == \"pre_event\":\n",
    "        y = make_pre_event_labels(df, args.ts_col, args.label_col,\n",
    "                                  delta_min=args.label_delta_minutes,\n",
    "                                  pos_dilate_back_steps=args.pos_dilate_back_steps,\n",
    "                                  resample_minutes=1)\n",
    "    else:\n",
    "        raise NotImplementedError(\"only pre_event supported\")\n",
    "\n",
    "    total_pos = int(y.sum()); total_all = len(y)\n",
    "    print(f\"[DEBUG] total samples={human_count(total_all)} | positives={human_count(total_pos)}\")\n",
    "\n",
    "    df, feature_cols = build_auto_features(df, base_feature_cols, args.ts_col, bool(args.auto_features))\n",
    "    seqs_all, _X_dummy, _ = build_windows(df, args.ts_col, feature_cols, y,\n",
    "                                          seq_len=args.seq_len, stride=args.stride,\n",
    "                                          pre_event_minutes=args.label_delta_minutes)\n",
    "\n",
    "    def _fit_transform_by_train_rows(tr_win_indices, feature_cols_in):\n",
    "        tr_point_mask = np.zeros(len(df), dtype=bool)\n",
    "        for i_idx in tr_win_indices:\n",
    "            s = seqs_all[i_idx]; tr_point_mask[s.start:s.end] = True\n",
    "        train_std = df.loc[tr_point_mask, feature_cols_in].std(numeric_only=True)\n",
    "        keep_cols = [c for c in feature_cols_in if (c in train_std.index and not pd.isna(train_std[c]) and train_std[c] > 0)]\n",
    "        dropped = [c for c in feature_cols_in if c not in keep_cols]\n",
    "        if dropped: print(f\"[PREP] drop zero-variance cols on train: {len(dropped)} dropped\")\n",
    "        train_mean = df.loc[tr_point_mask, keep_cols].mean(numeric_only=True)\n",
    "        df[keep_cols] = df[keep_cols].fillna(train_mean)\n",
    "        sc_mean = df.loc[tr_point_mask, keep_cols].mean()\n",
    "        sc_std  = df.loc[tr_point_mask, keep_cols].std().replace(0, 1.0)\n",
    "        df[keep_cols] = (df[keep_cols] - sc_mean) / sc_std\n",
    "        X_all_local = df[keep_cols].values.astype(np.float32)\n",
    "        return keep_cols, X_all_local, sc_mean.to_dict(), sc_std.to_dict()\n",
    "\n",
    "    if args.split_mode == \"event\":\n",
    "        tr_idx_arr, ho_idx_arr = event_based_split(df, args.ts_col, y, seqs_all,\n",
    "                                                   pre_event_minutes=args.label_delta_minutes,\n",
    "                                                   holdout_frac=args.event_holdout_frac,\n",
    "                                                   holdout_min_events=args.event_holdout_min,\n",
    "                                                   seed=args.seed,\n",
    "                                                   extra_min=args.event_holdout_extra_min,\n",
    "                                                   target_holdout_frac=args.target_holdout_frac,\n",
    "                                                   target_holdout_max=args.target_holdout_max,\n",
    "                                                   pos_dilate_back_steps=args.pos_dilate_back_steps,\n",
    "                                                   tail_min=args.event_holdout_tail_min)\n",
    "        if tr_idx_arr is None:\n",
    "            tr_mask_pts, ho_mask_pts = embargo_posaware_split(df, args.ts_col, y,\n",
    "                holdout_days=args.val_holdout_days, embargo_minutes=args.embargo_minutes,\n",
    "                min_pos_train=args.min_pos_train, min_pos_holdout=args.min_pos_holdout,\n",
    "                seqs_for_split=seqs_all, min_train_windows=32, min_holdout_windows=32,\n",
    "                target_train_frac=args.target_train_frac, train_frac_weight=args.train_frac_weight)\n",
    "            tr_idx = [i for i, s in enumerate(seqs_all) if tr_mask_pts[s.start] and tr_mask_pts[s.end - 1]]\n",
    "            ho_idx = [i for i, s in enumerate(seqs_all) if ho_mask_pts[s.start] and ho_mask_pts[s.end - 1]]\n",
    "            feature_cols, X_all, sc_mean, sc_std = _fit_transform_by_train_rows(tr_idx, feature_cols)\n",
    "            tr_seqs = [seqs_all[i] for i in tr_idx]; ho_seqs = [seqs_all[i] for i in ho_idx]\n",
    "        else:\n",
    "            feature_cols, X_all, sc_mean, sc_std = _fit_transform_by_train_rows(tr_idx_arr.tolist(), feature_cols)\n",
    "            tr_seqs = [seqs_all[i] for i in tr_idx_arr.tolist()]\n",
    "            ho_seqs = [seqs_all[i] for i in ho_idx_arr.tolist()]\n",
    "    else:\n",
    "        tr_mask_pts, ho_mask_pts = embargo_posaware_split(df, args.ts_col, y,\n",
    "            holdout_days=args.val_holdout_days, embargo_minutes=args.embargo_minutes,\n",
    "            min_pos_train=args.min_pos_train, min_pos_holdout=args.min_pos_holdout,\n",
    "            seqs_for_split=seqs_all, min_train_windows=32, min_holdout_windows=32,\n",
    "            target_train_frac=args.target_train_frac, train_frac_weight=args.train_frac_weight)\n",
    "        tr_idx = [i for i, s in enumerate(seqs_all) if tr_mask_pts[s.start] and tr_mask_pts[s.end - 1]]\n",
    "        ho_idx = [i for i, s in enumerate(seqs_all) if ho_mask_pts[s.start] and ho_mask_pts[s.end - 1]]\n",
    "        feature_cols, X_all, sc_mean, sc_std = _fit_transform_by_train_rows(tr_idx, feature_cols)\n",
    "        tr_seqs = [seqs_all[i] for i in tr_idx]; ho_seqs = [seqs_all[i] for i in ho_idx]\n",
    "\n",
    "    y_tr = np.array([s.label for s in tr_seqs], dtype=int)\n",
    "    y_ho = np.array([s.label for s in ho_seqs], dtype=int)\n",
    "    print(f\"[SPLIT-FINAL] train windows={human_count(len(tr_seqs))} (pos={human_count(int(y_tr.sum()))}) | holdout windows={human_count(len(ho_seqs))} (pos={human_count(int(y_ho.sum()))})\")\n",
    "    if y_tr.sum() == 0: print(\"[WARN] Train has 0 positives.\")\n",
    "    if y_ho.sum() == 0: print(\"[WARN] Holdout has 0 positives. AP/threshold selection may be invalid.\")\n",
    "\n",
    "    X_all = df[feature_cols].values.astype(np.float32)\n",
    "    ds_tr = WindowDataset(X_all, tr_seqs); ds_ho = WindowDataset(X_all, ho_seqs)\n",
    "    sampler = HardNegPosSampler(tr_seqs, batch_size=args.batch_size,\n",
    "                                pos_oversample=args.pos_oversample,\n",
    "                                hard_neg_frac=args.hard_neg_frac,\n",
    "                                neg_frac=args.neg_frac, shuffle=True)\n",
    "    dl_tr = DataLoader(ds_tr, batch_sampler=sampler, num_workers=0)\n",
    "    dl_ho = DataLoader(ds_ho, batch_size=args.batch_size, shuffle=False, drop_last=False, num_workers=0)\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    in_dim = len(feature_cols)\n",
    "    if args.model == \"gru\":\n",
    "        model = GRUModel(in_dim, hidden=args.hidden, layers=args.layers, dropout=args.dropout)\n",
    "    else:\n",
    "        channels = parse_channels(args.tcn_channels)\n",
    "        model = TCNModel(in_dim, channels=channels, dropout=args.dropout)\n",
    "    model.to(device)\n",
    "\n",
    "    npos = int(y_tr.sum()); nneg = int(len(y_tr) - npos)\n",
    "    pos_weight = (nneg / max(1, npos)) if npos > 0 else 1.0\n",
    "    alpha_pos = (nneg / max(1, (nneg + npos))) if (args.alpha < 0 and (nneg + npos) > 0) else float(max(0.0, args.alpha))\n",
    "    print(f\"[INFO] Focal params: gamma={args.gamma}, alpha_pos={alpha_pos:.4f}, pos_weight={pos_weight:.3f}\")\n",
    "    focal_loss = FocalLossWithLogits(gamma=args.gamma, alpha_pos=alpha_pos,\n",
    "                                     label_smoothing=args.label_smoothing, pos_weight=pos_weight)\n",
    "    optim = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, T_max=max(20, args.epochs // 2), eta_min=3e-6)\n",
    "\n",
    "    warmup_epochs = min(5, max(1, args.epochs // 5))\n",
    "    best_ap, best_y_hold, best_p_hold = -1.0, None, None\n",
    "    patience = max(3, int(args.early_stop_patience))\n",
    "    bad = 0\n",
    "\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        mixup = args.mixup_alpha if epoch > warmup_epochs else 0.0\n",
    "        jitter = args.jitter_std if epoch > warmup_epochs else 0.0\n",
    "        cutout = args.time_cutout_prob if epoch > warmup_epochs else 0.0\n",
    "        tr_loss = train_one_epoch(model, dl_tr, optim, focal_loss, device,\n",
    "                                  mixup_alpha=mixup, jitter_std=jitter, time_cutout_prob=cutout,\n",
    "                                  loss_mode=args.loss_mode, rank_weight=args.rank_weight, rank_margin=args.rank_margin, args=args)\n",
    "        ap, y_hold_cur, p_hold_cur = evaluate(model, dl_ho, device, args=args)\n",
    "        ap_str = \"nan\" if (ap is None or (isinstance(ap, float) and np.isnan(ap))) else f\"{ap:.6f}\"\n",
    "        print(f\"Epoch {epoch:03d} | train_loss={tr_loss:.6f} | AP={ap_str}\")\n",
    "\n",
    "        improved = False\n",
    "        if args.use_best_ap and (ap is not None) and not (isinstance(ap, float) and np.isnan(ap)):\n",
    "            if ap > best_ap + 1e-4:\n",
    "                best_ap = ap; best_y_hold = y_hold_cur.copy(); best_p_hold = p_hold_cur.copy(); improved = True\n",
    "        scheduler.step()\n",
    "        if improved: bad = 0\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= args.early_stop_patience:\n",
    "                print(f\"[EARLY-STOP] patience={args.early_stop_patience}, best_AP={best_ap:.6f}\")\n",
    "                break\n",
    "\n",
    "    final_y, final_p = (best_y_hold, best_p_hold) if (args.use_best_ap and (best_y_hold is not None)) \\\n",
    "                        else evaluate(model, dl_ho, device, args=args)[1:3]\n",
    "\n",
    "    if final_y is not None and final_p is not None and final_y.sum() >= 0:\n",
    "        if args.calibration and args.calibration != \"none\":\n",
    "            final_p = calibrate_probs(final_p, final_y, method=args.calibration)\n",
    "        if args.save_probs:\n",
    "            y_arr = np.asarray(final_y).reshape(-1).astype(int)\n",
    "            p_arr = np.asarray(final_p).reshape(-1).astype(float)\n",
    "            assert len(y_arr) == len(p_arr), f\"len mismatch y={len(y_arr)} p={len(p_arr)}\"\n",
    "            out = pd.DataFrame({\"y_true\": y_arr, \"p_raw\": p_arr, \"p_hat\": p_arr})\n",
    "            out.to_csv(args.probs_path, index=False)\n",
    "            print(f\"[SAVE] holdout probabilities → {args.probs_path}\")\n",
    "        _save_eval_plots_and_report(final_y, final_p, args)\n",
    "\n",
    "    if int(getattr(args, \"save_artifacts\", 1)) == 1:\n",
    "        artifact = {\n",
    "            \"model_type\": args.model,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"feature_cols\": feature_cols,\n",
    "            \"scaler_mean\": sc_mean,\n",
    "            \"scaler_std\": sc_std,\n",
    "            \"args\": vars(args),\n",
    "            \"pytorch_version\": torch.__version__,\n",
    "            \"seed\": args.seed,\n",
    "        }\n",
    "        torch.save(artifact, args.artifacts_path)\n",
    "        print(f\"[SAVE] model artifacts (single file) → {args.artifacts_path}\")\n",
    "\n",
    "    print(\"[DONE] training pipeline completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fc213dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] total samples=17,265 | positives=2,286\n",
      "[SPLIT] cutoff=2020-10-30 13:08:44 | embargo=30min | train_win=8806 (pos=1599) | holdout_win=7502 (pos=328) | OK (nudged)\n",
      "[PREP] drop zero-variance cols on train: 18 dropped\n",
      "[SPLIT-FINAL] train windows=8,806 (pos=1,599) | holdout windows=7,502 (pos=328)\n",
      "[INFO] Focal params: gamma=2.0, alpha_pos=0.8184, pos_weight=4.507\n",
      "Epoch 001 | train_loss=0.022143 | AP=0.955793\n",
      "Epoch 002 | train_loss=0.001164 | AP=0.934562\n",
      "Epoch 003 | train_loss=0.000512 | AP=0.882187\n",
      "[EARLY-STOP] patience=2, best_AP=0.955793\n",
      "[SAVE] holdout probabilities → dataset/holdout_probs.csv\n",
      "[EVALS] Saved: evals/*.png, metrics.txt\n",
      "[SAVE] model artifacts (single file) → models/model_artifacts.pkl\n",
      "[DONE] training pipeline completed.\n"
     ]
    }
   ],
   "source": [
    "# ▶️ 실행 셀 (노트북에서 돌릴 때만 이 셀 실행)\n",
    "# 여기서 argparse에 들어갈 인자를 수동으로 지정해준다.\n",
    "import sys\n",
    "\n",
    "# 필요한 대로 바꿔서 써라\n",
    "sys.argv = [\n",
    "    \"train_deep_tcn_clean_v2_evals.py\",\n",
    "    \"--input\", \"dataset/train_features_60m.csv\",\n",
    "    \"--model\", \"tcn\",\n",
    "    \"--tcn-channels\", \"64,64,96\",\n",
    "    \"--resample\", \"\",\n",
    "    \"--seq-len\", \"300\",\n",
    "    \"--stride\", \"1\",\n",
    "    \"--label-mode\", \"pre_event\",\n",
    "    \"--label-delta-minutes\", \"180\",\n",
    "    \"--pos-dilate-back-steps\", \"6\",\n",
    "    \"--split-mode\", \"time\",\n",
    "    \"--val-holdout-days\", \"1.5\",\n",
    "    \"--embargo-minutes\", \"30\",\n",
    "    \"--min-pos-holdout\", \"300\",\n",
    "    \"--target-holdout-frac\", \"0.20\",\n",
    "    \"--target-holdout-max\", \"0.35\",\n",
    "    \"--batch-size\", \"128\",\n",
    "    \"--epochs\", \"20\",\n",
    "    \"--lr\", \"5e-5\",\n",
    "    \"--weight-decay\", \"1e-4\",\n",
    "    \"--dropout\", \"0.25\",\n",
    "    \"--pos-oversample\", \"6\",\n",
    "    \"--neg-frac\", \"0.3\",\n",
    "    \"--hard-neg-frac\", \"0.7\",\n",
    "    \"--label-smoothing\", \"0.01\",\n",
    "    \"--loss-mode\", \"focal+rank\",\n",
    "    \"--rank-weight\", \"0.20\",\n",
    "    \"--rank-margin\", \"0.15\",\n",
    "    \"--post-k\", \"4\",\n",
    "    \"--ema-span\", \"6\",\n",
    "    \"--open-th\", \"0.57\",\n",
    "    \"--close-th\", \"0.33\",\n",
    "    \"--cooldown-min\", \"120\",\n",
    "    \"--early-stop-patience\", \"2\",\n",
    "    \"--auto-features\", \"0\",\n",
    "    \"--calibration\", \"isotonic\",\n",
    "    \"--th-objective\", \"event_f1\",\n",
    "    \"--precision-floor\", \"0.90\",\n",
    "    \"--fp-budget\", \"40\",\n",
    "    \"--save-probs\", \"1\",\n",
    "    \"--probs-path\", \"dataset/holdout_probs.csv\",\n",
    "    \"--flip-logits\", \"1\",\n",
    "    \"--seed\", \"42\",\n",
    "    \"--use-best-ap\"\n",
    "]\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "56c40c8d-ead9-4330-8809-1411b613b084",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 노트북 위치 기준으로 evals 경로 맞춰줌\n",
    "base_dir = os.path.dirname(os.path.abspath(__file__)) if \"__file__\" in globals() else os.getcwd()\n",
    "img_dir = os.path.join(base_dir, \"evals\")\n",
    "\n",
    "plots = [\n",
    "    \"pr_curve.png\",\n",
    "    \"roc_curve.png\",\n",
    "    \"calibration_curve.png\",\n",
    "    \"threshold_sweep.png\",\n",
    "    \"prob_hist.png\",\n",
    "]\n",
    "\n",
    "for name in plots:\n",
    "    path = os.path.join(img_dir, name)\n",
    "    if os.path.exists(path):\n",
    "        img = Image.open(path)\n",
    "        plt.figure(figsize=(5, 4))\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(name)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"[WARN] {path} 없어서 스킵함\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2787153-8af3-4be2-8b3a-ae0c52c0dee9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch 2.4 (GPU)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
