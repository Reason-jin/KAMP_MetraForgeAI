{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e82126da",
   "metadata": {},
   "source": [
    "# KAMP Pipeline (Lite) â€” Notebook Version\n",
    "\n",
    "KAMP ì†Œì„±ê°€ê³µ í’ˆì§ˆë³´ì¦ AI â€” End-to-End íŒŒì´í”„ë¼ì¸ (ë¼ì´íŠ¸)\n",
    "=======================================================\n",
    "ëª©ì \n",
    "----\n",
    "- **ê²€ì¦ ì‚°ì¶œë¬¼(results/*) ì „ë¶€ ì œê±°**í•˜ê³ , ëª¨ë¸ë§ì— í•„ìš”í•œ íŒŒì¼ë§Œ ìƒì„±í•œë‹¤.\n",
    "- ìµœì¢… ì‚°ì¶œë¬¼ì€ **dataset/** í´ë”ì— ë”± 2ê°œ:\n",
    "  1) `final_prepared.csv` â€” *í’€ë²„ì „ê³¼ ê²°ê³¼ê°€ 1:1ë¡œ ë™ì¼*\n",
    "  2) `train_features_{window}m.csv` â€” rolling-window í”¼ì²˜\n",
    "\n",
    "ì‹¤í–‰ ì˜ˆì‹œ(í•œ ì¤„)\n",
    "----------------\n",
    "python kamp_pipeline_lite.py --stage all --input \"2. ì†Œì„±ê°€ê³µ í’ˆì§ˆë³´ì¦ AI ë°ì´í„°ì…‹.csv\" --dataset-dir dataset --freq 5s --normalize-labels --drop-na-labels --window 60 --features-output dataset/train_features_60m.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe17f21",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fe0dfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc97db6",
   "metadata": {},
   "source": [
    "## Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c584845e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_dir(p: str | Path) -> Path:\n",
    "    p = Path(p); p.mkdir(parents=True, exist_ok=True); return p\n",
    "\n",
    "def read_frame(path: str | Path) -> pd.DataFrame:\n",
    "    path = str(path)\n",
    "    if path.lower().endswith((\".parquet\", \".pq\")):\n",
    "        return pd.read_parquet(path)\n",
    "    return pd.read_csv(path, encoding=\"utf-8-sig\")\n",
    "\n",
    "def write_df(df: pd.DataFrame, out_path: Path):\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if out_path.suffix.lower() in {\".parquet\", \".pq\"}:\n",
    "        df.to_parquet(out_path, index=False)\n",
    "    else:\n",
    "        df.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "def log(msg: str):\n",
    "    print(f\"[KAMP-LITE] {msg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189e8751",
   "metadata": {},
   "source": [
    "## Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b080a440",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_parser():\n",
    "    p = argparse.ArgumentParser(description=\"KAMP End-to-End Pipeline (Lite)\")\n",
    "    # ë‹¨ê³„ ì„ íƒ\n",
    "    p.add_argument(\"--stage\", choices=[\"preprocess\", \"features\", \"all\"], required=True,\n",
    "                   help=\"ì‹¤í–‰ ë‹¨ê³„ ì„ íƒ: preprocess | features | all\")\n",
    "    # ì „ì²˜ë¦¬ ì˜µì…˜ (í’€ë²„ì „ê³¼ ë™ì¼ ë¡œì§ ë³´ì¥)\n",
    "    p.add_argument(\"--input\", default=\"2. ì†Œì„±ê°€ê³µ í’ˆì§ˆë³´ì¦ AI ë°ì´í„°ì…‹.csv\", help=\"ì…ë ¥ CSV/Parquet ê²½ë¡œ\")\n",
    "    p.add_argument(\"--dataset-dir\", default=\"dataset\", help=\"ëª¨ë¸ë§ìš© ìµœì¢… CSV ì €ì¥ í´ë”\")\n",
    "    p.add_argument(\"--label-col\", default=\"passorfail\", help=\"ë¼ë²¨ ì»¬ëŸ¼ëª…\")\n",
    "    p.add_argument(\"--freq\", default=\"5s\", help=\"ë¦¬ìƒ˜í”Œ ê°„ê²©(ì˜ˆ: 5s, 1min). ë‚´ë¶€ì—ì„œ ì†Œë¬¸ì ê°•ì œ\")\n",
    "    p.add_argument(\"--gap-thr\", type=float, default=10.0, help=\"ëˆ„ë½ gap ì„ê³„(ì´ˆ)\")\n",
    "    p.add_argument(\"--normalize-labels\", action=\"store_true\", help=\"ë¼ë²¨ pass/fail/True/False â†’ 1/0 ì •ê·œí™”\")\n",
    "    p.add_argument(\"--drop-na-labels\", action=\"store_true\", help=\"ì •ê·œí™” í›„ ë¼ë²¨ NaN í–‰ ì œê±°\")\n",
    "    p.add_argument(\"--make-splits\", action=\"store_true\", help=\"dataset/train.csv,val.csv,test.csv ë¶„í•  ìƒì„±\")\n",
    "    p.add_argument(\"--split\", default=\"0.8,0.1,0.1\", help=\"train,val,test ë¹„ìœ¨ (í•©=1)\")\n",
    "    # í”¼ì²˜ ì˜µì…˜\n",
    "    p.add_argument(\"--features-input\", default=\"dataset/final_prepared.csv\",\n",
    "                   help=\"í”¼ì²˜ ìƒì„± ì…ë ¥ CSV (ê¸°ë³¸: dataset/final_prepared.csv)\")\n",
    "    p.add_argument(\"--window\", type=int, default=60, help=\"rolling window í¬ê¸°(ë¶„ ë‹¨ìœ„)\")\n",
    "    p.add_argument(\"--features-output\", default=None,\n",
    "                   help=\"í”¼ì²˜ ìƒì„± ê²°ê³¼ CSV (ê¸°ë³¸: dataset/train_features_{window}m.csv)\")\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f8840d",
   "metadata": {},
   "source": [
    "## Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "540ccb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step0_clean_like_full(input_path: str | Path, label_col: str = \"passorfail\",\n",
    "                          gap_thr: float = 10.0, force_grid: bool = True, freq: str = \"5s\"):\n",
    "    \"\"\"ì‹œê°„ ì •ë ¬ â†’ ì¤‘ë³µ í‰ê·  ë³‘í•© â†’ ê·œì¹™ê²©ì ë¦¬ìƒ˜í”Œ(ffill 1-step). (í’€ë²„ì „ê³¼ ë™ì¼)\n",
    "    ë°˜í™˜: (df_all, df_labeled) â€” ë‘˜ ë‹¤ datetime í¬í•¨. íŒŒì¼ ì €ì¥ ì—†ìŒ(ë¼ì´íŠ¸ ì •ì±…).\n",
    "    \"\"\"\n",
    "    log(\"STEP0: ì‹œê°„ ì •ë ¬ ë° í´ë Œì§• (full ë™ì‘ ë³µì œ)\")\n",
    "    df = read_frame(input_path)\n",
    "\n",
    "    # datetime íŒŒì‹± (date ë˜ëŠ” datetime ì—´ ì§€ì›)\n",
    "    if \"date\" in df.columns:\n",
    "        dt = pd.to_datetime(df[\"date\"], format=\"%Y %m %d %H:%M:%S\", errors=\"coerce\")\n",
    "        df = df.drop(columns=[\"date\"])  # ì›ë³¸ date ì œê±°\n",
    "    elif \"datetime\" in df.columns:\n",
    "        dt = pd.to_datetime(df[\"datetime\"], errors=\"coerce\")\n",
    "        df = df.drop(columns=[\"datetime\"])  # ìƒˆë¡œ ë„£ê¸° ìœ„í•´ ì œê±°\n",
    "    else:\n",
    "        raise KeyError(\"ì…ë ¥ ë°ì´í„°ì— 'date' ë˜ëŠ” 'datetime' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    df.insert(0, \"datetime\", dt)\n",
    "    df = df.sort_values(\"datetime\").reset_index(drop=True)\n",
    "\n",
    "    # ì¤‘ë³µ timestamp í‰ê·  ë³‘í•©(ìˆ«ìì—´ë§Œ)\n",
    "    if df[\"datetime\"].duplicated().any():\n",
    "        df = df.groupby(\"datetime\", as_index=False).mean(numeric_only=True)\n",
    "        log(\"ì¤‘ë³µ timestamp â†’ í‰ê·  ë³‘í•©\")\n",
    "\n",
    "    # ëˆ„ë½ gap ê²€ì¦(ë¼ì´íŠ¸ëŠ” íŒŒì¼ ë¯¸ì €ì¥)\n",
    "    df[\"time_diff\"] = df[\"datetime\"].diff().dt.total_seconds()\n",
    "\n",
    "    # ê·œì¹™ ê²©ì ë³´ì • (freq ì†Œë¬¸ì ê°•ì œ, ffill limit=1)\n",
    "    df = df.set_index(\"datetime\").sort_index()\n",
    "    if force_grid:\n",
    "        freq = str(freq).lower()\n",
    "        full_idx = pd.date_range(df.index.min(), df.index.max(), freq=freq)\n",
    "        df = df.reindex(full_idx)\n",
    "        df = df.ffill(limit=1)\n",
    "        df.index.name = \"datetime\"\n",
    "        log(f\"{freq} ê²©ì ë³´ì • + ffill(limit=1)\")\n",
    "\n",
    "    # ë¼ë²¨ ì¡´ì¬ ê¸°ì¤€ ë¶„ê¸°(í’€ë²„ì „ê³¼ ë™ì¼)\n",
    "    has_label = df[label_col].notna() if label_col in df.columns else pd.Series(True, index=df.index)\n",
    "    df_labeled = df[has_label].reset_index()\n",
    "    df_all = df.reset_index()\n",
    "    return df_all, df_labeled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd322002",
   "metadata": {},
   "source": [
    "## Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1797194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step4_feature_grouping_like_full(df_labeled: pd.DataFrame, label_col: str = \"passorfail\") -> pd.DataFrame:\n",
    "    log(\"STEP4: ê·¸ë£¹í•‘ ë° í”¼ì²˜ ìƒì„± (full ë™ì‘ ë³µì œ)\")\n",
    "    groups = {\n",
    "        \"G1_ZoneTemp\": [\"EX1.Z1_PV\", \"EX1.Z2_PV\", \"EX1.Z3_PV\", \"EX1.Z4_PV\"],\n",
    "        \"G2_HeadTemp\": [\"EX1.H1_PV\", \"EX1.H2_PV\", \"EX1.H3_PV\", \"EX1.H4_PV\", \"EX1.A1_PV\", \"EX1.A2_PV\"],\n",
    "        \"G3_MeltTemp\": [\"EX2.MELT_TEMP\", \"EX3.MELT_TEMP\", \"EX4.MELT_TEMP\", \"EX5.MELT_TEMP\"],\n",
    "        \"G4_PressureTorque\": [\"EX1.MELT_P_PV\", \"EX1.MD_PV\", \"EX1.MD_TQ\"],\n",
    "        \"G5_CoolFlow\": [\"EX1.H2O_PV\"],\n",
    "    }\n",
    "    df = df_labeled.copy()\n",
    "    dt_series = pd.to_datetime(df[\"datetime\"], errors=\"coerce\") if \"datetime\" in df.columns else None\n",
    "    num_df = df.select_dtypes(include=[np.number])\n",
    "    feats = pd.DataFrame(index=num_df.index)\n",
    "    for gname, cols in groups.items():\n",
    "        existing = [c for c in cols if c in num_df.columns]\n",
    "        if not existing:\n",
    "            continue\n",
    "        sub = num_df[existing]\n",
    "        feats[f\"{gname}_mean\"] = sub.mean(axis=1)\n",
    "        feats[f\"{gname}_std\"] = sub.std(axis=1, ddof=0)\n",
    "        feats[f\"{gname}_range\"] = sub.max(axis=1) - sub.min(axis=1)\n",
    "    if label_col in num_df.columns:\n",
    "        feats[label_col] = num_df[label_col].values\n",
    "    else:\n",
    "        feats[label_col] = np.nan\n",
    "    if dt_series is not None:\n",
    "        feats.insert(0, \"datetime\", dt_series.values)\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfecb40b",
   "metadata": {},
   "source": [
    "## Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af1bd29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_labels_inplace(df: pd.DataFrame, label_col: str) -> None:\n",
    "    if label_col in df.columns:\n",
    "        df[label_col] = (\n",
    "            df[label_col]\n",
    "            .replace({\"pass\":1,\"Pass\":1,\"PASS\":1, True:1,\n",
    "                      \"fail\":0,\"Fail\":0,\"FAIL\":0, False:0})\n",
    "            .pipe(pd.to_numeric, errors=\"coerce\")\n",
    "        )\n",
    "\n",
    "def save_dataset_like_full(feats: pd.DataFrame, dataset_dir: Path, normalize_labels: bool, drop_na_labels: bool, label_col: str):\n",
    "    df = feats.copy()\n",
    "    if normalize_labels:\n",
    "        normalize_labels_inplace(df, label_col)\n",
    "        log(\"ë¼ë²¨ ì •ê·œí™” ì™„ë£Œ (pass/fail â†’ 1/0)\")\n",
    "    if drop_na_labels and label_col in df.columns:\n",
    "        n0 = len(df)\n",
    "        df = df.dropna(subset=[label_col])\n",
    "        log(f\"ë¼ë²¨ NaN ì œê±°: {n0 - len(df)}í–‰ drop\")\n",
    "    write_df(df, dataset_dir / \"final_prepared.csv\")\n",
    "    log(\"ì €ì¥: dataset/final_prepared.csv\")\n",
    "    return df\n",
    "\n",
    "def make_splits_like_full(df: pd.DataFrame, dataset_dir: Path, ratio_str: str = \"0.8,0.1,0.1\"):\n",
    "    r = [float(x) for x in ratio_str.split(\",\")]\n",
    "    assert abs(sum(r) - 1.0) < 1e-6, \"--split ë¹„ìœ¨ í•©ì€ 1ì´ì–´ì•¼ í•¨\"\n",
    "    n = len(df)\n",
    "    n_train = int(n * r[0])\n",
    "    n_val = int(n * r[1])\n",
    "    train = df.iloc[:n_train]\n",
    "    val = df.iloc[n_train:n_train + n_val]\n",
    "    test = df.iloc[n_train + n_val:]\n",
    "    write_df(train, dataset_dir / \"train.csv\")\n",
    "    write_df(val, dataset_dir / \"val.csv\")\n",
    "    write_df(test, dataset_dir / \"test.csv\")\n",
    "    log(f\"splits â†’ train:{len(train)}, val:{len(val)}, test:{len(test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab18c7ad",
   "metadata": {},
   "source": [
    "## Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bcf77690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _safe_numeric_cols(df):\n",
    "    drop = {\"datetime\", \"passorfail\"}\n",
    "    num_cols = [c for c in df.columns if c not in drop and np.issubdtype(df[c].dtype, np.number)]\n",
    "    return num_cols\n",
    "\n",
    "def _time_index(df):\n",
    "    if \"datetime\" not in df.columns:\n",
    "        raise ValueError(\"`datetime` ì»¬ëŸ¼ì´ í•„ìš”í•¨.\")\n",
    "    df = df.copy()\n",
    "    df[\"datetime\"] = pd.to_datetime(df[\"datetime\"], errors=\"coerce\")\n",
    "    if df[\"datetime\"].isna().any():\n",
    "        df = df.dropna(subset=[\"datetime\"])  # ê¹¨ì§„ ë‚ ì§œ drop\n",
    "    df = df.sort_values(\"datetime\").set_index(\"datetime\")\n",
    "    return df\n",
    "\n",
    "def _rolling_features(df, cols, window_min):\n",
    "    from scipy.stats import linregress\n",
    "    w = f\"{window_min}min\"\n",
    "    out = pd.DataFrame(index=df.index)\n",
    "    agg_funcs = {\"mean\":\"mean\",\"std\":\"std\",\"min\":\"min\",\"max\":\"max\",\"skew\":\"skew\",\"kurt\":\"kurt\"}\n",
    "    for stat_name, func in agg_funcs.items():\n",
    "        tmp = df[cols].rolling(w, min_periods=2).agg(func)\n",
    "        tmp.columns = [f\"{c}__{stat_name}__{window_min}m\" for c in tmp.columns]\n",
    "        out = out.join(tmp)\n",
    "    rmax = df[cols].rolling(w, min_periods=2).max()\n",
    "    rmin = df[cols].rolling(w, min_periods=2).min()\n",
    "    rng = rmax - rmin; rng.columns = [f\"{c}__range__{window_min}m\" for c in cols]\n",
    "    out = out.join(rng)\n",
    "    first = df[cols].rolling(w, min_periods=2).apply(lambda x: x.iloc[0], raw=False)\n",
    "    delta = df[cols] - first; delta.columns = [f\"{c}__delta__{window_min}m\" for c in cols]\n",
    "    out = out.join(delta)\n",
    "    def slope_over_time(y, times):\n",
    "        if len(y) < 2: return np.nan\n",
    "        t = (times.view(\"int64\") // 10**9)\n",
    "        return linregress(t, y).slope\n",
    "    slope_df = pd.DataFrame(index=df.index)\n",
    "    idx = df.index\n",
    "    for c in cols:\n",
    "        series = df[c]\n",
    "        vals = []\n",
    "        for i in range(len(series)):\n",
    "            end_time = idx[i]\n",
    "            start_time = end_time - pd.Timedelta(minutes=window_min)\n",
    "            win = series.loc[start_time:end_time]\n",
    "            vals.append(slope_over_time(win.values, win.index) if win.shape[0] >= 2 else np.nan)\n",
    "        slope_df[c] = vals\n",
    "    slope_df.columns = [f\"{c}__slope__{window_min}m\" for c in cols]\n",
    "    out = out.join(slope_df)\n",
    "    return out\n",
    "\n",
    "def run_features(features_input: str | Path, window_min: int, features_output: str | None):\n",
    "    inp = Path(features_input)\n",
    "    if not inp.exists():\n",
    "        raise FileNotFoundError(f\"ì…ë ¥ íŒŒì¼ ì—†ìŒ: {inp}\")\n",
    "    out_path = Path(features_output or f\"dataset/train_features_{window_min}m.csv\")\n",
    "    base = pd.read_csv(inp)\n",
    "    if \"passorfail\" not in base.columns:\n",
    "        raise ValueError(\"`passorfail` ë¼ë²¨ ì»¬ëŸ¼ì´ í•„ìš”í•¨.\")\n",
    "    df_idxed = _time_index(base)\n",
    "    lab = df_idxed[\"passorfail\"].replace({\"pass\":1,\"Pass\":1,\"PASS\":1, True:1,\n",
    "                                          \"fail\":0,\"Fail\":0,\"FAIL\":0, False:0})\n",
    "    lab = pd.to_numeric(lab, errors=\"coerce\")\n",
    "    mask = lab.notna()\n",
    "    df_idxed = df_idxed.loc[mask]\n",
    "    label = lab.loc[mask].astype(int)\n",
    "    num_cols = _safe_numeric_cols(df_idxed.reset_index())\n",
    "    if len(num_cols) == 0:\n",
    "        raise ValueError(\"ìˆ˜ì¹˜ í”¼ì²˜ ì»¬ëŸ¼ì„ ì°¾ì§€ ëª»í•¨. ì…ë ¥ CSVì— numeric í”¼ì²˜ê°€ ìˆëŠ”ì§€ í™•ì¸.\")\n",
    "    feats = _rolling_features(df_idxed, num_cols, window_min)\n",
    "    out = feats.copy(); out[\"datetime\"] = out.index; out = out.reset_index(drop=True)\n",
    "    ydf = label.to_frame(\"passorfail\").copy(); ydf[\"datetime\"] = ydf.index; ydf = ydf.reset_index(drop=True)\n",
    "    out = pd.merge_asof(out.sort_values(\"datetime\"), ydf.sort_values(\"datetime\"), on=\"datetime\", direction=\"nearest\")\n",
    "    if out.isna().mean().mean() > 0.3:\n",
    "        out = out.dropna()\n",
    "    out.to_csv(out_path, index=False, encoding=\"utf-8-sig\", date_format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    log(f\"âœ… Saved features: {out_path}  (rows={len(out)}, cols={out.shape[1]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a52cf02",
   "metadata": {},
   "source": [
    "## Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9635b87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    args = build_parser().parse_args()\n",
    "    dataset_dir = ensure_dir(args.dataset_dir)\n",
    "\n",
    "    if args.stage in (\"preprocess\", \"all\"):\n",
    "        df_all, df_labeled = step0_clean_like_full(\n",
    "            input_path=args.input,\n",
    "            label_col=args.label_col,\n",
    "            gap_thr=args.gap_thr,\n",
    "            force_grid=True,\n",
    "            freq=args.freq,\n",
    "        )\n",
    "        feats = step4_feature_grouping_like_full(df_labeled, label_col=args.label_col)\n",
    "        final = save_dataset_like_full(\n",
    "            feats,\n",
    "            dataset_dir=dataset_dir,\n",
    "            normalize_labels=args.normalize_labels,\n",
    "            drop_na_labels=args.drop_na_labels,\n",
    "            label_col=args.label_col,\n",
    "        )\n",
    "        if args.make_splits:\n",
    "            make_splits_like_full(final, dataset_dir, args.split)\n",
    "        log(\"[stage=preprocess] ì™„ë£Œ: dataset/final_prepared.csv\")\n",
    "\n",
    "    if args.stage in (\"features\", \"all\"):\n",
    "        run_features(\n",
    "            features_input=args.features_input,\n",
    "            window_min=args.window,\n",
    "            features_output=args.features_output,\n",
    "        )\n",
    "        log(\"[stage=features] ì™„ë£Œ: rolling window features ì €ì¥\")\n",
    "\n",
    "    log(\"âœ… LITE íŒŒì´í”„ë¼ì¸ ì¢…ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c2d18a",
   "metadata": {},
   "source": [
    "## Notebook Runner (ì‹¤í–‰ì€ ì—¬ê¸°ì„œ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ae1fda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KAMP-LITE] STEP0: ì‹œê°„ ì •ë ¬ ë° í´ë Œì§• (full ë™ì‘ ë³µì œ)\n",
      "[KAMP-LITE] 5s ê²©ì ë³´ì • + ffill(limit=1)\n",
      "[KAMP-LITE] STEP4: ê·¸ë£¹í•‘ ë° í”¼ì²˜ ìƒì„± (full ë™ì‘ ë³µì œ)\n",
      "[KAMP-LITE] ë¼ë²¨ ì •ê·œí™” ì™„ë£Œ (pass/fail â†’ 1/0)\n",
      "[KAMP-LITE] ë¼ë²¨ NaN ì œê±°: 0í–‰ drop\n",
      "[KAMP-LITE] ì €ì¥: dataset/final_prepared.csv\n",
      "[KAMP-LITE] âœ… Saved features: dataset/train_features_60m.csv  (rows=17265, cols=137)\n",
      "âœ… Notebook run ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# ğŸš€ Notebook Runner (no argparse)\n",
    "# ì•„ë˜ ë³€ìˆ˜ë§Œ ê³ ì³ì„œ ë°”ë¡œ ì‹¤í–‰í•´ë¼.\n",
    "\n",
    "STAGE = \"all\"                     # \"preprocess\", \"features\", \"all\"\n",
    "INPUT = \"2. ì†Œì„±ê°€ê³µ í’ˆì§ˆë³´ì¦ AI ë°ì´í„°ì…‹.csv\"  # ì›ë³¸ ë°ì´í„°\n",
    "DATASET_DIR = \"dataset\"           # ì €ì¥ í´ë”\n",
    "LABEL_COL = \"passorfail\"\n",
    "FREQ = \"5s\"\n",
    "GAP_THR = 10.0\n",
    "NORMALIZE_LABELS = True\n",
    "DROP_NA_LABELS = True\n",
    "MAKE_SPLITS = False\n",
    "SPLIT = \"0.8,0.1,0.1\"\n",
    "\n",
    "FEATURES_INPUT = \"dataset/final_prepared.csv\"\n",
    "WINDOW = 60                       # minutes\n",
    "FEATURES_OUTPUT = \"dataset/train_features_60m.csv\"\n",
    "\n",
    "# â†“â†“â†“ ì´ ì…€ì€ ì•„ë˜ ì •ì˜ëœ í•¨ìˆ˜ë“¤ì´ ì´ë¯¸ ìˆì–´ì•¼ í•œë‹¤. (ì•„ë˜ ì…€ ì „ë¶€ í•œë²ˆ ì‹¤í–‰í•˜ê³  ì™€ë„ ë¨)\n",
    "try:\n",
    "    dataset_dir = ensure_dir(DATASET_DIR)\n",
    "\n",
    "    if STAGE in (\"preprocess\", \"all\"):\n",
    "        df_all, df_labeled = step0_clean_like_full(\n",
    "            input_path=INPUT,\n",
    "            label_col=LABEL_COL,\n",
    "            gap_thr=GAP_THR,\n",
    "            force_grid=True,\n",
    "            freq=FREQ,\n",
    "        )\n",
    "        feats = step4_feature_grouping_like_full(df_labeled, label_col=LABEL_COL)\n",
    "        final = save_dataset_like_full(\n",
    "            feats,\n",
    "            dataset_dir=dataset_dir,\n",
    "            normalize_labels=NORMALIZE_LABELS,\n",
    "            drop_na_labels=DROP_NA_LABELS,\n",
    "            label_col=LABEL_COL,\n",
    "        )\n",
    "        if MAKE_SPLITS:\n",
    "            make_splits_like_full(final, dataset_dir, SPLIT)\n",
    "\n",
    "    if STAGE in (\"features\", \"all\"):\n",
    "        run_features(\n",
    "            features_input=FEATURES_INPUT,\n",
    "            window_min=WINDOW,\n",
    "            features_output=FEATURES_OUTPUT,\n",
    "        )\n",
    "\n",
    "    print(\"âœ… Notebook run ì™„ë£Œ\")\n",
    "except NameError as e:\n",
    "    print(\"âš ï¸ ë¨¼ì € ì•„ë˜ ì½”ë“œì…€ë“¤(Imports/í•¨ìˆ˜ì •ì˜)ì„ ì „ë¶€ ì‹¤í–‰í•œ ë‹¤ìŒ ì´ ì…€ì„ ë‹¤ì‹œ ì‹¤í–‰í•´.\")\n",
    "    print(\"NameError:\", e)\n",
    "except Exception as e:\n",
    "    print(\"âŒ ì˜¤ë¥˜ ë°œìƒ:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa0f37f-28db-4897-becc-d99a51b0b083",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fc6ed3-c3f8-4540-9d04-540bb34262c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch 2.4 (GPU)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
